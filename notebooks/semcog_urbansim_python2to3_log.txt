(base) da@da-virtual-machine:~$ 2to3 --output-dir=semcog_urbansim3 -W -n semcog_urbansim
WARNING: --write-unchanged-files/-W implies -w.
lib2to3.main: Output in 'semcog_urbansim3' will mirror the input directory 'semcog_urbansim' layout.
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored semcog_urbansim/Cost_Shift_test.py
--- semcog_urbansim/Cost_Shift_test.py	(original)
+++ semcog_urbansim/Cost_Shift_test.py	(refactored)
@@ -8,7 +8,7 @@
 import output_indicators
 
 data_out = os.path.join(misc.runs_dir(), "cost_shift_%d.h5" % misc.get_run_number())
-print data_out
+print(data_out)
 
 orca.run(["refiner",
           'build_networks',
@@ -39,7 +39,7 @@
     "refiner",
     # "travel_model", Fixme: on hold
 ],
-    iter_vars=range(2016, 2025 + 1),
+    iter_vars=list(range(2016, 2025 + 1)),
     data_out=data_out,
     out_base_tables=['jobs', 'base_job_space', 'employment_sectors', 'annual_relocation_rates_for_jobs',
                      'households', 'persons', 'annual_relocation_rates_for_households',
RefactoringTool: Writing converted semcog_urbansim/Cost_Shift_test.py to semcog_urbansim3/Cost_Shift_test.py.
RefactoringTool: Refactored semcog_urbansim/Refiner.py
--- semcog_urbansim/Refiner.py	(original)
+++ semcog_urbansim/Refiner.py	(refactored)
@@ -11,12 +11,12 @@
 orca.add_table('refiner_events', pd.read_csv("data/add_pop_11032017.csv"))
 
 data_out = utils.get_run_filename()
-print data_out
+print(data_out)
 
 orca.run([
     "refiner",
 ],
-    iter_vars=range(2015, 2015 + 1),
+    iter_vars=list(range(2015, 2015 + 1)),
     data_out=data_out,
     out_base_tables=['jobs', 'base_job_space', 'employment_sectors', 'annual_relocation_rates_for_jobs',
                      'households', 'persons', 'annual_relocation_rates_for_households',
RefactoringTool: Writing converted semcog_urbansim/Refiner.py to semcog_urbansim3/Refiner.py.
RefactoringTool: Refactored semcog_urbansim/Simulation.py
--- semcog_urbansim/Simulation.py	(original)
+++ semcog_urbansim/Simulation.py	(refactored)
@@ -8,14 +8,14 @@
 import output_indicators
 
 data_out = utils.get_run_filename()
-print data_out
+print(data_out)
 
 import sys,statvfs
 
 f = os.statvfs("/home")
 freespace=f[statvfs.F_BAVAIL] * f[statvfs.F_BSIZE] / (1048576 * 1024.0)
 if freespace < 10:
-    print freespace, 'GB available. Disk space is too small, stop running'
+    print(freespace, 'GB available. Disk space is too small, stop running')
     sys.exit()
 
 
@@ -48,7 +48,7 @@
     "gq_pop_scaling_model",
     # "travel_model", Fixme: on hold
 ],
-    iter_vars=range(2016, 2045 + 1),
+    iter_vars=list(range(2016, 2045 + 1)),
     data_out=data_out,
     out_base_tables=['jobs', 'base_job_space', 'employment_sectors', 'annual_relocation_rates_for_jobs',
                      'households', 'persons', 'annual_relocation_rates_for_households',
RefactoringTool: Writing converted semcog_urbansim/Simulation.py to semcog_urbansim3/Simulation.py.
RefactoringTool: Refactored semcog_urbansim/Simulation_no_scaling.py
--- semcog_urbansim/Simulation_no_scaling.py	(original)
+++ semcog_urbansim/Simulation_no_scaling.py	(refactored)
@@ -8,7 +8,7 @@
 import output_indicators
 
 data_out = utils.get_run_filename()
-print data_out
+print(data_out)
 
 orca.run(["refiner",
           'build_networks',
@@ -39,7 +39,7 @@
     "refiner",
     # "travel_model", Fixme: on hold
 ],
-    iter_vars=range(2016, 2045 + 1),
+    iter_vars=list(range(2016, 2045 + 1)),
     data_out=data_out,
     out_base_tables=['jobs', 'base_job_space', 'base_job_space', 'employment_sectors', 'annual_relocation_rates_for_jobs',
                      'households', 'persons', 'annual_relocation_rates_for_households',
RefactoringTool: Writing converted semcog_urbansim/Simulation_no_scaling.py to semcog_urbansim3/Simulation_no_scaling.py.
RefactoringTool: Refactored semcog_urbansim/assumptions.py
--- semcog_urbansim/assumptions.py	(original)
+++ semcog_urbansim/assumptions.py	(refactored)
@@ -88,7 +88,7 @@
 })
 
 seed = 271828
-print "using seed", seed
+print("using seed", seed)
 random.seed(seed)
 pd.np.random.seed(seed)
 
RefactoringTool: Writing converted semcog_urbansim/assumptions.py to semcog_urbansim3/assumptions.py.
RefactoringTool: No changes to semcog_urbansim/dataset.py
RefactoringTool: Writing converted semcog_urbansim/dataset.py to semcog_urbansim3/dataset.py.
RefactoringTool: Refactored semcog_urbansim/lcm_utils.py
--- semcog_urbansim/lcm_utils.py	(original)
+++ semcog_urbansim/lcm_utils.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 
 import os
 import copy
@@ -475,7 +475,7 @@
     with open(os.path.join(misc.configs_dir(), 'model_structure.yaml')) as f:
         model_category_configs = yaml.load(f)['models']
 
-    for model_category, category_attributes in model_category_configs.items():
+    for model_category, category_attributes in list(model_category_configs.items()):
         category_attributes['config_filenames'] = yaml_configs[model_category]
 
     return model_category_configs
RefactoringTool: Writing converted semcog_urbansim/lcm_utils.py to semcog_urbansim3/lcm_utils.py.
RefactoringTool: Refactored semcog_urbansim/models.py
--- semcog_urbansim/models.py	(original)
+++ semcog_urbansim/models.py	(refactored)
@@ -14,6 +14,7 @@
 import utils
 import lcm_utils
 import variables
+from functools import reduce
 
 # Set up location choice model objects.
 # Register as injectable to be used throughout simulation
@@ -21,7 +22,7 @@
 hlcm_step_names = []
 elcm_step_names = []
 model_configs = lcm_utils.get_model_category_configs()
-for model_category_name, model_category_attributes in model_configs.items():
+for model_category_name, model_category_attributes in list(model_configs.items()):
     if model_category_attributes['model_type'] == 'location_choice':
         model_config_files = model_category_attributes['config_filenames']
 
@@ -40,7 +41,7 @@
 orca.add_injectable('hlcm_step_names', sorted(hlcm_step_names, reverse=True))
 orca.add_injectable('elcm_step_names', sorted(elcm_step_names, reverse=True))
 
-for name, model in location_choice_models.items():
+for name, model in list(location_choice_models.items()):
     lcm_utils.register_choice_model_step(model.name,
                                          model.choosers,
                                          choice_function=lcm_utils.unit_choices)
@@ -85,7 +86,7 @@
     def func():
         buildings = orca.get_table('buildings')
         nodes_walk = orca.get_table('nodes_walk')
-        print yaml_file
+        print(yaml_file)
         return utils.hedonic_simulate(yaml_file, buildings,
                                       nodes_walk, dep_var)
 
@@ -126,7 +127,7 @@
     relocation_rates.probability_of_relocating *= .2
     reloc = relocation.RelocationModel(relocation_rates, 'probability_of_relocating')
     _print_number_unplaced(households, 'building_id')
-    print "un-placing"
+    print("un-placing")
     hh = households.to_frame(households.local_columns)
     idx_reloc = reloc.find_movers(hh)
     households.update_col_from_series('building_id',
@@ -140,7 +141,7 @@
     relocation_rates = annual_relocation_rates_for_jobs.to_frame().reset_index()
     reloc = relocation.RelocationModel(relocation_rates, 'job_relocation_probability')
     _print_number_unplaced(jobs, 'building_id')
-    print "un-placing"
+    print("un-placing")
     j = jobs.to_frame(jobs.local_columns)
     idx_reloc = reloc.find_movers(j[j.home_based_status <= 0])
     jobs.update_col_from_series('building_id',
@@ -149,7 +150,8 @@
     _print_number_unplaced(jobs, 'building_id')
 
 
-def presses_trans((ct, hh, p, target, iter_var)):
+def presses_trans(xxx_todo_changeme1):
+    (ct, hh, p, target, iter_var) = xxx_todo_changeme1
     ct_finite = ct[ct.persons_max <= 100]
     ct_inf = ct[ct.persons_max > 100]
     tran = transition.TabularTotalsTransition(ct_finite, 'total_number_of_households')
@@ -191,14 +193,15 @@
     region_p = persons.to_frame(persons.local_columns)
     region_target = remi_pop_total.to_frame()
 
-    def cut_to_la((large_area_id, hh)):
+    def cut_to_la(xxx_todo_changeme):
+        (large_area_id, hh) = xxx_todo_changeme
         p = region_p[region_p.household_id.isin(hh.index)]
         target = int(region_target.loc[large_area_id, str(iter_var)])
         ct = region_ct[region_ct.large_area_id == large_area_id]
         del ct["large_area_id"]
         return ct, hh, p, target, iter_var
 
-    arg_per_la = map(cut_to_la, region_hh.groupby('large_area_id'))
+    arg_per_la = list(map(cut_to_la, region_hh.groupby('large_area_id')))
     # cunks_per_la = map(presses_trans, arg_per_la)
     pool = Pool(8)
     cunks_per_la = pool.map(presses_trans, arg_per_la)
@@ -216,13 +219,13 @@
         hh = hh.reset_index()
         hh['household_id_old'] = hh['household_id']
         new_hh = (hh.building_id == -1).sum()
-        hh.loc[hh.building_id == -1, 'household_id'] = range(hhidmax, hhidmax + new_hh)
+        hh.loc[hh.building_id == -1, 'household_id'] = list(range(hhidmax, hhidmax + new_hh))
         hhidmax += new_hh
         hhid_map = hh[['household_id_old', 'household_id']].set_index('household_id_old')
         p.index.name = 'person_id'
         p = pd.merge(p.reset_index(), hhid_map, left_on='household_id', right_index=True)
         new_p = (p.household_id_x != p.household_id_y).sum()
-        p.loc[p.household_id_x != p.household_id_y, 'person_id'] = range(pidmax, pidmax + new_p)
+        p.loc[p.household_id_x != p.household_id_y, 'person_id'] = list(range(pidmax, pidmax + new_p))
         pidmax += new_p
         p['household_id'] = p['household_id_y']
 
@@ -494,7 +497,7 @@
         return len(local_agents) - number_of_agents
 
     for tid, trecords in refinements.groupby("transaction_id"):
-        print '** processing transcaction ', tid
+        print('** processing transcaction ', tid)
         agent_types = trecords.agents.drop_duplicates()
         assert len(agent_types) == 1, "different agents in same transaction_id"
         agent_type = agent_types.iloc[0]
@@ -502,7 +505,7 @@
         pool = pd.DataFrame(data=None, columns=agents.columns)
 
         for _, record in trecords[trecords.action == 'clone'].iterrows():
-            print record
+            print(record)
             agents, pool = clone_agents(agents,
                                         pool,
                                         record.agent_expression,
@@ -510,7 +513,7 @@
                                         record.amount)
 
         for _, record in trecords[trecords.action == 'subtract_pop'].iterrows():
-            print record
+            print(record)
             assert agent_type == 'households'
             agents, pool = subtract_pop_agents(agents,
                                                pool,
@@ -519,7 +522,7 @@
                                                record.amount)
 
         for _, record in trecords[trecords.action == 'subtract'].iterrows():
-            print record
+            print(record)
             agents, pool = subtract_agents(agents,
                                            pool,
                                            record.agent_expression,
@@ -527,7 +530,7 @@
                                            record.amount)
 
         for _, record in trecords[trecords.action == 'add_pop'].iterrows():
-            print record
+            print(record)
             assert agent_type == 'households'
             agents, pool = add_pop_agents(agents,
                                           pool,
@@ -536,7 +539,7 @@
                                           record.amount)
 
         for _, record in trecords[trecords.action == 'add'].iterrows():
-            print record
+            print(record)
             agents, pool = add_agents(agents,
                                       pool,
                                       record.agent_expression,
@@ -544,7 +547,7 @@
                                       record.amount)
 
         for _, record in trecords[trecords.action == 'target_pop'].iterrows():
-            print record
+            print(record)
             assert agent_type == 'households'
             diff = target_agents(dic_agent[record.agents],
                                  record.agent_expression,
@@ -565,20 +568,20 @@
                                                    diff)
 
         for _, record in trecords[trecords.action == 'target'].iterrows():
-            print record
+            print(record)
             diff = target_agents(dic_agent[record.agents],
                                  record.agent_expression,
                                  record.location_expression,
                                  record.amount)
             if diff < 0:
-                print 'add: ', abs(diff)
+                print('add: ', abs(diff))
                 agents, pool = add_agents(agents,
                                           pool,
                                           record.agent_expression,
                                           record.location_expression,
                                           abs(diff))
             elif diff > 0:
-                print 'subtract: ', abs(diff)
+                print('subtract: ', abs(diff))
                 agents, pool = subtract_agents(agents,
                                                pool,
                                                record.agent_expression,
@@ -611,7 +614,7 @@
         hh_index_lookup.columns = ['household_id']
         p = pd.merge(persons.reset_index(), hh_index_lookup, left_on='household_id', right_index=True)
         new_p = (p.household_id_x != p.household_id_y).sum()
-        p.loc[p.household_id_x != p.household_id_y, 'person_id'] = range(pidmax, pidmax + new_p)
+        p.loc[p.household_id_x != p.household_id_y, 'person_id'] = list(range(pidmax, pidmax + new_p))
         p['household_id'] = p['household_id_y']
         persons = p.set_index('person_id')
 
@@ -688,7 +691,7 @@
     buildings_idx = []
 
     def sample(targets, type_b, accounting, weights):
-        for b_city_id, target in targets[targets > 0].iteritems():
+        for b_city_id, target in targets[targets > 0].items():
             rel_b = type_b[type_b.b_city_id == b_city_id]
             rel_b = rel_b[rel_b[accounting] <= target]
             size = min(len(rel_b), int(target))
@@ -817,7 +820,7 @@
     form_to_btype_dists = orca.get_injectable("form_btype_distributions")
     btype_dists = form_to_btype_dists[form]
     # keys() and values() guaranteed to be in same order
-    btype = np.random.choice(a=btype_dists.keys(), p=btype_dists.values())
+    btype = np.random.choice(a=list(btype_dists.keys()), p=list(btype_dists.values()))
     return btype
 
 
@@ -842,7 +845,7 @@
     """
     form_to_btype = orca.get_injectable("form_to_btype")
     form_btype_dists = {}
-    for form in form_to_btype.keys():
+    for form in list(form_to_btype.keys()):
         bldgs = buildings.loc[buildings.building_type_id
             .isin(form_to_btype[form])]
         bldgs_by_type = bldgs.groupby('building_type_id').size()
@@ -862,15 +865,15 @@
     copied form parcel_utils and modified
     """
     from developer import develop
-    print 'processing large area id:', lid
+    print('processing large area id:', lid)
     cfg = misc.config(cfg)
     dev = develop.Developer.from_yaml(orca.get_table('feasibility_' + str(lid)).to_frame(), forms,
                                       target_units, parcel_size,
                                       ave_unit_size, current_units,
                                       orca.get_injectable('year'), str_or_buffer=cfg)
 
-    print("{:,} feasible buildings before running developer".format(
-        len(dev.feasibility)))
+    print(("{:,} feasible buildings before running developer".format(
+        len(dev.feasibility))))
 
     new_buildings = dev.pick(profit_to_prob_func, custom_selection_func)
     orca.add_table('feasibility_' + str(lid), dev.feasibility)
@@ -989,7 +992,7 @@
     idx_invalid_building_id = np.in1d(j.building_id, b.index.values) == False
 
     if idx_invalid_building_id.sum() > 0:
-        print("we have jobs with bad building id's there are #", idx_invalid_building_id.sum())
+        print(("we have jobs with bad building id's there are #", idx_invalid_building_id.sum()))
         j.loc[idx_invalid_building_id, 'building_id'] = np.random.choice(
             b.index.values,
             idx_invalid_building_id.sum())
@@ -998,7 +1001,7 @@
         orca.add_table("jobs", j)
     idx_invalid_building_id = np.in1d(h.building_id, b.index.values) == False
     if idx_invalid_building_id.sum() > 0:
-        print("we have households with bad building id's there are #", idx_invalid_building_id.sum())
+        print(("we have households with bad building id's there are #", idx_invalid_building_id.sum()))
         h.loc[idx_invalid_building_id, 'building_id'] = np.random.choice(
             b.index.values,
             idx_invalid_building_id.sum())
@@ -1237,4 +1240,4 @@
     of unplaced agents.
     """
     counts = (df[fieldname] == -1).sum()
-    print "Total currently unplaced: %d" % counts
+    print("Total currently unplaced: %d" % counts)
RefactoringTool: Writing converted semcog_urbansim/models.py to semcog_urbansim3/models.py.
RefactoringTool: Refactored semcog_urbansim/output_indicators.py
--- semcog_urbansim/output_indicators.py	(original)
+++ semcog_urbansim/output_indicators.py	(refactored)
@@ -17,11 +17,11 @@
             df = hdf[name]
         else:
             stub_name = str(2020) + '/' + tbl
-            print "No table named " + name + ". Using the structuer from " + stub_name + "."
+            print("No table named " + name + ". Using the structuer from " + stub_name + ".")
             df = hdf[stub_name].iloc[0:0]
 
         if tbl in {'households', 'jobs'} and 'large_area_id' not in df.columns:
-            print 'impute large_area_id'
+            print('impute large_area_id')
             df['large_area_id'] = misc.reindex(orca.get_table('buildings').large_area_id, df.building_id)
 
         orca.add_table(tbl, df.fillna(0))
@@ -344,7 +344,7 @@
             df = df.to_frame([hh_name, gq_name]).fillna(0)
             return df[hh_name] + df[gq_name]
 
-    for (a, b) in [(00, 04), (05, 17), (18, 24), (18, 64), (25, 34), (35, 64), (65, pd.np.inf),
+    for (a, b) in [(00, 0o4), (0o5, 17), (18, 24), (18, 64), (25, 34), (35, 64), (65, pd.np.inf),
                    (00, 17), (25, 44), (45, 64), (65, 84), (85, pd.np.inf),
                    (35, 59), (60, 64), (65, 74), (75, pd.np.inf)]:
         make_pop_age(a, b)
@@ -407,9 +407,9 @@
         runnum.write(os.path.basename(os.path.normpath(outdir)))
 
     store_la = pd.HDFStore(run_name, mode='r')
-    print store_la
-
-    spacing = int(30.0 / len(set(j[1: 5] for j in store_la.keys()) & set(str(i) for i in range(2016, 2045 + 1))))
+    print(store_la)
+
+    spacing = int(30.0 / len(set(j[1: 5] for j in list(store_la.keys())) & set(str(i) for i in range(2016, 2045 + 1))))
     if spacing == 1:
         all_years_dir = os.path.join(outdir, 'annual')
         if not (os.path.exists(all_years_dir)):
@@ -509,7 +509,7 @@
 
         # geo level: school district
 
-    years = range(2015, 2045 + 1, spacing)
+    years = list(range(2015, 2045 + 1, spacing))
     year_names = ["yr" + str(i) for i in years]
     indicators = ['hh', 'hh_pop', 'gq_pop', 'pop',
                   'housing_units', 'hu_filter', 'parcel_is_allowed_residential', 'parcel_is_allowed_demolition',
@@ -572,12 +572,12 @@
     start = time.clock()
     dict_ind = defaultdict(list)
     for year in years:
-        print 'processing ', year
+        print('processing ', year)
         orca_year_dataset(store_la, year)
         for tab in geom:
             dict_ind[tab].append(orca.get_table(tab).to_frame(indicators))
     end = time.clock()
-    print "runtime:", end - start
+    print("runtime:", end - start)
 
     # region should have same value no matter how you slice it.
     df = pd.DataFrame()
@@ -594,12 +594,12 @@
     df = df[df.columns[~df.columns.str.startswith('pct_')]]
 
     sumstd = df.groupby(level=0).std().sum().sort_values()
-    print sumstd[sumstd > 0]
-
-    print df[sumstd[sumstd > 0].index]
+    print(sumstd[sumstd > 0])
+
+    print(df[sumstd[sumstd > 0].index])
 
     # Todo: add part of fenton to semmcd table
-    print set(orca.get_table('semmcds').to_frame(indicators).hh_pop.index) ^ set(orca.get_table('semmcds').hh_pop.index)
+    print(set(orca.get_table('semmcds').to_frame(indicators).hh_pop.index) ^ set(orca.get_table('semmcds').hh_pop.index))
 
     start = time.clock()
 
@@ -635,12 +635,12 @@
         whatnots_output[year_names[::5]].to_csv(os.path.join(outdir, "whatnots_output.csv"))
     whatnots_output.to_csv(os.path.join(all_years_dir, "whatnots_output.csv"))
     end = time.clock()
-    print "runtime whatnots:", end - start
+    print("runtime whatnots:", end - start)
 
     start = time.clock()
     geom = ['cities', 'large_areas', 'semmcds', 'zones']
     for tab in geom:
-        print tab
+        print(tab)
 
         if spacing == 1:
             writer = pd.ExcelWriter(os.path.join(outdir, tab + "_by_indicator_for_year.xlsx"))
@@ -681,12 +681,12 @@
                     df["large_area_name"] = name
                     df.set_index("large_area_name", append=True, inplace=True)
                 if len(df.columns) > 0:
-                    print "saving:", ind
+                    print("saving:", ind)
                     df = df.fillna(0)
                     df = df.sort_index().sort_index(1)
                     df.to_excel(writer, ind)
                 else:
-                    print "somtning is wrong with:", ind
+                    print("somtning is wrong with:", ind)
             writer.save()
 
         writer = pd.ExcelWriter(os.path.join(all_years_dir, tab + "_by_year_for_indicator.xlsx"))
@@ -710,19 +710,19 @@
                 df["large_area_name"] = name
                 df.set_index("large_area_name", append=True, inplace=True)
             if len(df.columns) > 0:
-                print "saving:", ind
+                print("saving:", ind)
                 df = df.fillna(0)
                 df = df.sort_index().sort_index(1)
                 df.to_excel(writer, ind)
             else:
-                print "somtning is wrong with:", ind
+                print("somtning is wrong with:", ind)
         writer.save()
     end = time.clock()
-    print "runtime geom:", end - start
+    print("runtime geom:", end - start)
 
     start = time.clock()
     for year in range(2015, 2046, 5):
-        print "buildings for", year
+        print("buildings for", year)
         orca_year_dataset(store_la, year)
         buildings = orca.get_table('buildings')
         df = buildings.to_frame(buildings.local_columns + ['city_id', 'large_area_id', 'x', 'y'])
@@ -743,14 +743,14 @@
         df = df.sort_index().sort_index(1)
         df.to_csv(os.path.join(outdir, "households_yr" + str(year) + ".csv"))
     end = time.clock()
-    print "runtime:", end - start
+    print("runtime:", end - start)
 
     start = time.clock()
     years = years[1:]
     year_names = ["yr" + str(i) for i in years]
     writer = pd.ExcelWriter(os.path.join(outdir, "buildings_dif_by_year.xlsx"))
     for year, year_name in zip(years, year_names):
-        print "buildings for", year
+        print("buildings for", year)
         orca_year_dataset(store_la, year)
         buildings = orca.get_table('buildings')
         df = buildings.to_frame(buildings.local_columns + ['city_id', 'large_area_id'])
@@ -768,6 +768,6 @@
 
     writer.save()
     end = time.clock()
-    print "runtime:", end - start
+    print("runtime:", end - start)
 
     store_la.close()
RefactoringTool: Writing converted semcog_urbansim/output_indicators.py to semcog_urbansim3/output_indicators.py.
RefactoringTool: Refactored semcog_urbansim/output_indicators_run_test.py
--- semcog_urbansim/output_indicators_run_test.py	(original)
+++ semcog_urbansim/output_indicators_run_test.py	(refactored)
@@ -10,10 +10,10 @@
 with open(os.path.join(os.getenv('DATA_HOME', "."), 'RUNNUM'), 'r') as f:
     start_num = int(f.read())
 
-for i in xrange(start_num, 1, -1):
+for i in range(start_num, 1, -1):
     data_out = os.path.join(misc.runs_dir(), "run%d.h5" % i)
     if os.path.isfile(data_out):
-        print "runnum:", i
+        print("runnum:", i)
         break
 
 data_out = './runs/run4032_hu_hh_base_2_gq_jobs_v5_hhpop_agegrp.h5'
RefactoringTool: Writing converted semcog_urbansim/output_indicators_run_test.py to semcog_urbansim3/output_indicators_run_test.py.
RefactoringTool: Refactored semcog_urbansim/proforma_generate_settings.py
--- semcog_urbansim/proforma_generate_settings.py	(original)
+++ semcog_urbansim/proforma_generate_settings.py	(refactored)
@@ -79,4 +79,4 @@
 
 # Test that pro forma can be loaded
 npf = SqFtProForma.from_yaml(str_or_buffer='configs/proforma.yaml')
-print(npf.get_debug_info('mixedoffice', 'deck'))
+print((npf.get_debug_info('mixedoffice', 'deck')))
RefactoringTool: Writing converted semcog_urbansim/proforma_generate_settings.py to semcog_urbansim3/proforma_generate_settings.py.
RefactoringTool: Refactored semcog_urbansim/proforma_run_test.py
--- semcog_urbansim/proforma_run_test.py	(original)
+++ semcog_urbansim/proforma_run_test.py	(refactored)
@@ -16,5 +16,5 @@
     "feasibility",  # compute development feasibility
     "residential_developer",  # build actual buildings
     "non_residential_developer"
-], iter_vars=range(2016, 2017),
+], iter_vars=list(range(2016, 2017)),
     out_interval=1)
RefactoringTool: Writing converted semcog_urbansim/proforma_run_test.py to semcog_urbansim3/proforma_run_test.py.
RefactoringTool: Refactored semcog_urbansim/refiner_run_test.py
--- semcog_urbansim/refiner_run_test.py	(original)
+++ semcog_urbansim/refiner_run_test.py	(refactored)
@@ -12,5 +12,5 @@
 orca.run([
     "neighborhood_vars",  # neighborhood variables
     "refiner",
-], iter_vars=range(2016, 2017),
+], iter_vars=list(range(2016, 2017)),
     out_interval=1)
RefactoringTool: Writing converted semcog_urbansim/refiner_run_test.py to semcog_urbansim3/refiner_run_test.py.
RefactoringTool: Refactored semcog_urbansim/transcad.py
--- semcog_urbansim/transcad.py	(original)
+++ semcog_urbansim/transcad.py	(refactored)
@@ -28,12 +28,12 @@
                   ["DataTable", datatable],
                   ["JoinField", joinfield]
               ]
-    print 'Importing UrbanSim data into Transcad'
+    print('Importing UrbanSim data into Transcad')
     transcad_operation('SEMCOGImportTabFile', dbname, macro_args)
     
     ####Run TM      
     loops = 1
-    print 'Running Transcad model'
+    print('Running Transcad model')
     transcad_operation('SEMCOG Run Loops', dbname, loops)
     
     #Import skims from travel model
@@ -42,21 +42,21 @@
                                   'AMTransitSkim':{'Generalized Cost':'generalized_cost'} }
     matrices = []
     row_index_name, col_index_name = "ZoneID", "ZoneID"  #default values
-    if matrix_attribute_name_map.has_key('row_index_name'):
+    if 'row_index_name' in matrix_attribute_name_map:
         row_index_name = matrix_attribute_name_map['row_index_name']
-    if matrix_attribute_name_map.has_key('col_index_name'):
+    if 'col_index_name' in matrix_attribute_name_map:
         col_index_name = matrix_attribute_name_map['col_index_name']
-    for key, val in matrix_attribute_name_map.iteritems():
+    for key, val in matrix_attribute_name_map.items():
         if (key != 'row_index_name') and (key != 'col_index_name'):
-            if val.has_key('row_index_name'):
+            if 'row_index_name' in val:
                 row_index_name = val['row_index_name']
-            if val.has_key('col_index_name'):
+            if 'col_index_name' in val:
                 col_index_name = val['col_index_name']
             matrix_file_name = file_locations[key]  #replace internal matrix name with absolute file name
-            matrices.append([matrix_file_name, row_index_name, col_index_name, val.items()])
+            matrices.append([matrix_file_name, row_index_name, col_index_name, list(val.items())])
     macro_args =[ ("ExportTo", tm_output_full_name) ]
     macro_args.append(("Matrix", matrices))
-    print 'Exporting matrices from Transcad to csv'
+    print('Exporting matrices from Transcad to csv')
     transcad_operation('SEMCOGExportMatrices', dbname, macro_args)
     
     #Read/process outputted csv
@@ -97,7 +97,7 @@
                         v = value  #string
                 return_dict[headers[col_index]].append(v)
         text_file.close()
-        for item, value in return_dict.iteritems():
+        for item, value in return_dict.items():
             try:
                 return_dict[item] = np.array(value)
             except:
RefactoringTool: Writing converted semcog_urbansim/transcad.py to semcog_urbansim3/transcad.py.
RefactoringTool: Refactored semcog_urbansim/utils.py
--- semcog_urbansim/utils.py	(original)
+++ semcog_urbansim/utils.py	(refactored)
@@ -23,7 +23,7 @@
 def change_scenario(scenario):
     assert scenario in orca.get_injectable("scenario_inputs"), \
         "Invalid scenario name"
-    print "Changing scenario to '%s'" % scenario
+    print("Changing scenario to '%s'" % scenario)
     orca.add_injectable("scenario", scenario)
 
 
@@ -56,8 +56,8 @@
         s_cnt = df[col].count()
         if df_cnt != s_cnt:
             fail = True
-            print "Found %d nas or inf (out of %d) in column %s" % \
-                  (df_cnt-s_cnt, df_cnt, col)
+            print("Found %d nas or inf (out of %d) in column %s" % \
+                  (df_cnt-s_cnt, df_cnt, col))
 
     assert not fail, "NAs were found in dataframe, please fix"
     return df
@@ -79,8 +79,8 @@
             val = df[fname].dropna().quantile()
         else:
             assert 0, "Fill type not found!"
-        print "Filling column {} with value {} ({} values)".\
-            format(fname, val, fill_cnt)
+        print("Filling column {} with value {} ({} values)".\
+            format(fname, val, fill_cnt))
         df[fname] = df[fname].fillna(val).astype(dtyp)
     return df
 
@@ -115,8 +115,8 @@
     price_or_rent, _ = yaml_to_class(cfg).predict_from_cfg(df, cfg)
 
     if price_or_rent.replace([np.inf, -np.inf], np.nan).isnull().sum() > 0:
-        print "Hedonic output %d nas or inf (out of %d) in column %s" % \
-              (price_or_rent.replace([np.inf, -np.inf], np.nan).isnull().sum(), len(price_or_rent), out_fname)
+        print("Hedonic output %d nas or inf (out of %d) in column %s" % \
+              (price_or_rent.replace([np.inf, -np.inf], np.nan).isnull().sum(), len(price_or_rent), out_fname))
     price_or_rent[price_or_rent > 700] = 700
     price_or_rent[price_or_rent < 1] = 1
     tbl.update_col_from_series(out_fname, price_or_rent)
@@ -160,23 +160,23 @@
     available_units = buildings[supply_fname]
     vacant_units = buildings[vacant_fname]
 
-    print "There are %d total available units" % available_units.sum()
-    print "    and %d total choosers" % len(choosers)
-    print "    but there are %d overfull buildings" % \
-          len(vacant_units[vacant_units < 0])
+    print("There are %d total available units" % available_units.sum())
+    print("    and %d total choosers" % len(choosers))
+    print("    but there are %d overfull buildings" % \
+          len(vacant_units[vacant_units < 0]))
 
     vacant_units = vacant_units[vacant_units > 0]
     units = locations_df.loc[np.repeat(vacant_units.index.values,
                              vacant_units.values.astype('int'))].reset_index()
 
-    print "    for a total of %d temporarily empty units" % vacant_units.sum()
-    print "    in %d buildings total in the region" % len(vacant_units)
+    print("    for a total of %d temporarily empty units" % vacant_units.sum())
+    print("    in %d buildings total in the region" % len(vacant_units))
 
     movers = choosers_df[choosers_df[out_fname] == -1]
 
     if len(movers) > vacant_units.sum():
-        print "WARNING: Not enough locations for movers"
-        print "    reducing locations to size of movers for performance gain"
+        print("WARNING: Not enough locations for movers")
+        print("    reducing locations to size of movers for performance gain")
         movers = movers.head(vacant_units.sum())
 
     new_units, _ = yaml_to_class(cfg).predict_from_cfg(movers, units, cfg)
@@ -193,15 +193,15 @@
     _print_number_unplaced(choosers, out_fname)
 
     vacant_units = buildings[vacant_fname]
-    print "    and there are now %d empty units" % vacant_units.sum()
-    print "    and %d overfull buildings" % len(vacant_units[vacant_units < 0])
+    print("    and there are now %d empty units" % vacant_units.sum())
+    print("    and %d overfull buildings" % len(vacant_units[vacant_units < 0]))
 
 
 def simple_relocation(choosers, relocation_rate, fieldname):
-    print "Total agents: %d" % len(choosers)
+    print("Total agents: %d" % len(choosers))
     _print_number_unplaced(choosers, fieldname)
 
-    print "Assinging for relocation..."
+    print("Assinging for relocation...")
     chooser_ids = np.random.choice(choosers.index, size=int(relocation_rate *
                                    len(choosers)), replace=False)
     choosers.update_col_from_series(fieldname,
@@ -214,17 +214,17 @@
     transition = GrowthRateTransition(rate)
     df = tbl.to_frame(tbl.local_columns)
 
-    print "%d agents before transition" % len(df.index)
+    print("%d agents before transition" % len(df.index))
     df, added, copied, removed = transition.transition(df, None)
-    print "%d agents after transition" % len(df.index)
+    print("%d agents after transition" % len(df.index))
 
     df.loc[added, location_fname] = -1
     orca.add_table(tbl.name, df)
 
 
 def _print_number_unplaced(df, fieldname):
-    print "Total currently unplaced: %d" % \
-          df[fieldname].value_counts().get(-1, 0)
+    print("Total currently unplaced: %d" % \
+          df[fieldname].value_counts().get(-1, 0))
 
 
 def random_choices(model, choosers, alternatives):
@@ -274,10 +274,10 @@
     vacant_units = alternatives[vacant_variable]
     vacant_units = vacant_units[vacant_units.index.values >= 0]  ## must have positive index 
 
-    print "There are %d total available units" % available_units.sum()
-    print "    and %d total choosers" % len(choosers)
-    print "    but there are %d overfull alternatives" % \
-          len(vacant_units[vacant_units < 0])
+    print("There are %d total available units" % available_units.sum())
+    print("    and %d total choosers" % len(choosers))
+    print("    but there are %d overfull alternatives" % \
+          len(vacant_units[vacant_units < 0]))
 
     vacant_units = vacant_units[vacant_units > 0]
 
@@ -288,20 +288,20 @@
     indexes = indexes[isin.values]
     units = alternatives.loc[indexes].reset_index()
 
-    print "    for a total of %d temporarily empty units" % vacant_units.sum()
-    print "    in %d alternatives total in the region" % len(vacant_units)
+    print("    for a total of %d temporarily empty units" % vacant_units.sum())
+    print("    in %d alternatives total in the region" % len(vacant_units))
 
     if missing > 0:
-        print "WARNING: %d indexes aren't found in the locations df -" % \
-            missing
-        print "    this is usually because of a few records that don't join "
-        print "    correctly between the locations df and the aggregations tables"
-
-    print "There are %d total movers for this LCM" % len(choosers)
+        print("WARNING: %d indexes aren't found in the locations df -" % \
+            missing)
+        print("    this is usually because of a few records that don't join ")
+        print("    correctly between the locations df and the aggregations tables")
+
+    print("There are %d total movers for this LCM" % len(choosers))
     
     if len(choosers) > vacant_units.sum():
-        print "WARNING: Not enough locations for movers"
-        print "    reducing locations to size of movers for performance gain"
+        print("WARNING: Not enough locations for movers")
+        print("    reducing locations to size of movers for performance gain")
         choosers = choosers.head(vacant_units.sum())
         
     choices = model.predict(choosers, units, debug=True)
@@ -403,7 +403,7 @@
         
         # By convention, choosers are denoted by a -1 value in the choice column
         choosers = choosers[choosers[self.choice_column] == -1]
-        print "%s agents are making a choice." % len(choosers)
+        print("%s agents are making a choice." % len(choosers))
 
         if choice_function:
             choices = choice_function(self, choosers, alternatives, **kwargs)
RefactoringTool: Writing converted semcog_urbansim/utils.py to semcog_urbansim3/utils.py.
RefactoringTool: Refactored semcog_urbansim/verify_data_structure.py
--- semcog_urbansim/verify_data_structure.py	(original)
+++ semcog_urbansim/verify_data_structure.py	(refactored)
@@ -5,7 +5,7 @@
 
 
 def order_rep(dumper, data):
-    return dumper.represent_mapping(u'tag:yaml.org,2002:map', data.items(), flow_style=False)
+    return dumper.represent_mapping('tag:yaml.org,2002:map', list(data.items()), flow_style=False)
 
 
 yaml.add_representer(OrderedDict, order_rep)
RefactoringTool: Writing converted semcog_urbansim/verify_data_structure.py to semcog_urbansim3/verify_data_structure.py.
RefactoringTool: Can't parse semcog_urbansim/data/fix_base_hh.py: ParseError: bad input: type=1, value='by', context=(' ', (115, 16))
RefactoringTool: Refactored semcog_urbansim/data/fix_base_hh.py
--- semcog_urbansim/data/fix_base_hh.py	(original)
+++ semcog_urbansim/data/fix_base_hh.py	(refactored)
@@ -1,474 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[2]:
-
-
-st = pd.HDFStore('all_semcog_data_01-30-18.h5')
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-# 2,370 hh pop needs to be taken out of Warren (3130) in base year (2015) 
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-pnothead = p.loc[(p.city_id==3130) & (p.relate>0)]
-
-
-# In[ ]:
-
-
-pnothead.shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.city_id <>3130)].shape
-
-
-# In[ ]:
-
-
-hhid = p.loc[(p.large_area_id==99) & (p.city_id <>3130)].sample(2370).household_id.values
-
-
-# In[ ]:
-
-
-hhid
-
-
-# In[ ]:
-
-
-p.loc[pnothead.sample(2370, replace=False).index.values, 'household_id']=hhid
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-st['persons_new']=p
-
-
-# In[ ]:
-
-
-st['persons'].shape
-
-
-# In[ ]:
-
-
-st['persons_new'].shape
-
-
-# In[ ]:
-
-
-h=st['households']
-
-
-# In[ ]:
-
-
-households: sum by persons, workers, childrens, 
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h = pd.merge(h, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-st['households'].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].count()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'children']=p.loc[(p.large_area_id==99) & (p.age<18)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'workers']=p.loc[(p.large_area_id==99) & (p.worker==1)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-st['households_new']=h
-
-
-# In[ ]:
-
-
-st['households_old']=st['households']
-
-
-# In[ ]:
-
-
-st['persons_old']=st['persons']
-
-
-# In[ ]:
-
-
-st['households_new'].columns
-
-
-# In[ ]:
-
-
-st['households']=st['households_new'][['building_id', 'cars', 'workers', 'persons', 'race_id', 'income',
-       'age_of_head', 'children']]
-
-
-# In[ ]:
-
-
-st['persons_new'].columns
-
-
-# In[ ]:
-
-
-st['persons']=st['persons_new'][['relate', 'age', 'worker', 'sex', 'race_id', 'member_id',
-       'household_id']]
-
-
-# In[ ]:
-
-
-st.close()
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-#check
-
-
-# In[ ]:
-
-
-st 
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons_old'], st['households_old'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-p.loc[p.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-pnew = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew = pd.merge(pnew, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-135794-133424
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-p.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-pnew.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99) & (pnew.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[4]:
-
-
-del st['households_old']
-
-
-# In[5]:
-
-
-del st['persons_old']
-
-
-# In[6]:
-
-
-del st['households_new']
-
-
-# In[7]:
-
-
-del st['persons_new']
-
-
-# In[25]:
-
-
-st.close()
-
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[4]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod2.h5')
-
-
-# In[8]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5', complevel=5, complib='zlib')
-
-
-# In[9]:
-
-
-for t in st.keys():
-    print t
-    st2[t]=st[t]
-
-
-# In[10]:
-
-
-st2.close()
-
-
-# In[11]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5')
-
-
-# In[12]:
-
-
-st
-
-
-# In[13]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod.h5')
-
-
-# In[14]:
-
-
-st
-
-
-# In[ ]:
-
-
-
-
+Non
RefactoringTool: Writing converted semcog_urbansim/data/fix_base_hh.py to semcog_urbansim3/data/fix_base_hh.py.
RefactoringTool: Can't parse semcog_urbansim/data/fix_city3130_hhsize-Copy1.py: ParseError: bad input: type=1, value='by', context=(' ', (157, 16))
RefactoringTool: Refactored semcog_urbansim/data/fix_city3130_hhsize-Copy1.py
--- semcog_urbansim/data/fix_city3130_hhsize-Copy1.py	(original)
+++ semcog_urbansim/data/fix_city3130_hhsize-Copy1.py	(refactored)
@@ -1,516 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[2]:
-
-
-st = pd.HDFStore('all_semcog_data_01-24-18.h5')
-
-
-# In[5]:
-
-
-hh = st['households']
-
-
-# In[16]:
-
-
-hh.fillna(0, inplace=True)
-
-
-# In[17]:
-
-
-hh.loc[(hh.index>990000000)& (hh.index<1150000000)].children.unique()
-
-
-# In[18]:
-
-
-hh.loc[(hh.index>990000000)& (hh.index<1150000000)].workers.unique()
-
-
-# In[19]:
-
-
-hh.loc[(hh.index>990000000)& (hh.index<1150000000)].persons.isnull().sum()
-
-
-# In[20]:
-
-
-st['households']=hh
-
-
-# In[21]:
-
-
-st.close()
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-# 2,370 hh pop needs to be taken out of Warren (3130) in base year (2015) 
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-pnothead = p.loc[(p.city_id==3130) & (p.relate>0)]
-
-
-# In[ ]:
-
-
-pnothead.shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.city_id <>3130)].shape
-
-
-# In[ ]:
-
-
-hhid = p.loc[(p.large_area_id==99) & (p.city_id <>3130)].sample(2370).household_id.values
-
-
-# In[ ]:
-
-
-hhid
-
-
-# In[ ]:
-
-
-p.loc[pnothead.sample(2370, replace=False).index.values, 'household_id']=hhid
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-st['persons_new']=p
-
-
-# In[ ]:
-
-
-st['persons'].shape
-
-
-# In[ ]:
-
-
-st['persons_new'].shape
-
-
-# In[ ]:
-
-
-h=st['households']
-
-
-# In[ ]:
-
-
-households: sum by persons, workers, childrens, 
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h = pd.merge(h, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-st['households'].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].count()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'children']=p.loc[(p.large_area_id==99) & (p.age<18)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'workers']=p.loc[(p.large_area_id==99) & (p.worker==1)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-st['households_new']=h
-
-
-# In[ ]:
-
-
-st['households_old']=st['households']
-
-
-# In[ ]:
-
-
-st['persons_old']=st['persons']
-
-
-# In[ ]:
-
-
-st['households_new'].columns
-
-
-# In[ ]:
-
-
-st['households']=st['households_new'][['building_id', 'cars', 'workers', 'persons', 'race_id', 'income',
-       'age_of_head', 'children']]
-
-
-# In[ ]:
-
-
-st['persons_new'].columns
-
-
-# In[ ]:
-
-
-st['persons']=st['persons_new'][['relate', 'age', 'worker', 'sex', 'race_id', 'member_id',
-       'household_id']]
-
-
-# In[ ]:
-
-
-st.close()
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-#check
-
-
-# In[ ]:
-
-
-st 
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons_old'], st['households_old'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-p.loc[p.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-pnew = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew = pd.merge(pnew, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-135794-133424
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-p.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-pnew.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99) & (pnew.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[4]:
-
-
-del st['households_old']
-
-
-# In[5]:
-
-
-del st['persons_old']
-
-
-# In[6]:
-
-
-del st['households_new']
-
-
-# In[7]:
-
-
-del st['persons_new']
-
-
-# In[25]:
-
-
-st.close()
-
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[4]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod2.h5')
-
-
-# In[8]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5', complevel=5, complib='zlib')
-
-
-# In[9]:
-
-
-for t in st.keys():
-    print t
-    st2[t]=st[t]
-
-
-# In[10]:
-
-
-st2.close()
-
-
-# In[11]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5')
-
-
-# In[12]:
-
-
-st
-
-
-# In[13]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod.h5')
-
-
-# In[14]:
-
-
-st
-
-
-# In[ ]:
-
-
-
-
+Non
RefactoringTool: Writing converted semcog_urbansim/data/fix_city3130_hhsize-Copy1.py to semcog_urbansim3/data/fix_city3130_hhsize-Copy1.py.
RefactoringTool: Can't parse semcog_urbansim/data/fix_city3130_hhsize.py: ParseError: bad input: type=1, value='by', context=(' ', (115, 16))
RefactoringTool: Refactored semcog_urbansim/data/fix_city3130_hhsize.py
--- semcog_urbansim/data/fix_city3130_hhsize.py	(original)
+++ semcog_urbansim/data/fix_city3130_hhsize.py	(refactored)
@@ -1,474 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[2]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod2.h5')
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-# 2,370 hh pop needs to be taken out of Warren (3130) in base year (2015) 
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-pnothead = p.loc[(p.city_id==3130) & (p.relate>0)]
-
-
-# In[ ]:
-
-
-pnothead.shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.city_id <>3130)].shape
-
-
-# In[ ]:
-
-
-hhid = p.loc[(p.large_area_id==99) & (p.city_id <>3130)].sample(2370).household_id.values
-
-
-# In[ ]:
-
-
-hhid
-
-
-# In[ ]:
-
-
-p.loc[pnothead.sample(2370, replace=False).index.values, 'household_id']=hhid
-
-
-# In[ ]:
-
-
-p.loc[(p.city_id==3130)].shape
-
-
-# In[ ]:
-
-
-st['persons_new']=p
-
-
-# In[ ]:
-
-
-st['persons'].shape
-
-
-# In[ ]:
-
-
-st['persons_new'].shape
-
-
-# In[ ]:
-
-
-h=st['households']
-
-
-# In[ ]:
-
-
-households: sum by persons, workers, childrens, 
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h = pd.merge(h, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-st['households'].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'persons']=p.loc[p.large_area_id==99].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].count()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'children']=p.loc[(p.large_area_id==99) & (p.age<18)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99].children.sum()
-
-
-# In[ ]:
-
-
-h.loc[h.large_area_id==99, 'workers']=p.loc[(p.large_area_id==99) & (p.worker==1)].groupby('household_id').size()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-h.workers.sum()
-
-
-# In[ ]:
-
-
-st['households_new']=h
-
-
-# In[ ]:
-
-
-st['households_old']=st['households']
-
-
-# In[ ]:
-
-
-st['persons_old']=st['persons']
-
-
-# In[ ]:
-
-
-st['households_new'].columns
-
-
-# In[ ]:
-
-
-st['households']=st['households_new'][['building_id', 'cars', 'workers', 'persons', 'race_id', 'income',
-       'age_of_head', 'children']]
-
-
-# In[ ]:
-
-
-st['persons_new'].columns
-
-
-# In[ ]:
-
-
-st['persons']=st['persons_new'][['relate', 'age', 'worker', 'sex', 'race_id', 'member_id',
-       'household_id']]
-
-
-# In[ ]:
-
-
-st.close()
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-#check
-
-
-# In[ ]:
-
-
-st 
-
-
-# In[ ]:
-
-
-p = pd.merge(st['persons_old'], st['households_old'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-b = pd.merge(st['buildings'], st['parcels'][['city_id','large_area_id']], left_on='parcel_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p = pd.merge(p, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-p.columns
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-p.loc[p.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-pnew = pd.merge(st['persons'], st['households'][['building_id']], left_on='household_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew = pd.merge(pnew, b[['city_id','large_area_id']], left_on='building_id',right_index=True, how='left')
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.city_id==3130].groupby('city_id').size()
-
-
-# In[ ]:
-
-
-135794-133424
-
-
-# In[ ]:
-
-
-p.loc[p.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-pnew.loc[pnew.large_area_id==99].shape
-
-
-# In[ ]:
-
-
-p.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-pnew.loc[p.age<18].shape
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99) & (p.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99) & (pnew.age<18)].shape
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-p.loc[(p.large_area_id==99)].worker.sum()
-
-
-# In[ ]:
-
-
-pnew.loc[(pnew.large_area_id==99)].worker.sum()
-
-
-# In[4]:
-
-
-del st['households_old']
-
-
-# In[5]:
-
-
-del st['persons_old']
-
-
-# In[6]:
-
-
-del st['households_new']
-
-
-# In[7]:
-
-
-del st['persons_new']
-
-
-# In[25]:
-
-
-st.close()
-
-
-# In[1]:
-
-
-import pandas as pd
-
-
-# In[4]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod2.h5')
-
-
-# In[8]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5', complevel=5, complib='zlib')
-
-
-# In[9]:
-
-
-for t in st.keys():
-    print t
-    st2[t]=st[t]
-
-
-# In[10]:
-
-
-st2.close()
-
-
-# In[11]:
-
-
-st2 = pd.HDFStore('all_semcog_data_01-24-18.h5')
-
-
-# In[12]:
-
-
-st
-
-
-# In[13]:
-
-
-st = pd.HDFStore('all_semcog_data_11-03-17_mod.h5')
-
-
-# In[14]:
-
-
-st
-
-
-# In[ ]:
-
-
-
-
+Non
RefactoringTool: Writing converted semcog_urbansim/data/fix_city3130_hhsize.py to semcog_urbansim3/data/fix_city3130_hhsize.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/Location Choice Model.py
--- semcog_urbansim/notebooks/Location Choice Model.py	(original)
+++ semcog_urbansim/notebooks/Location Choice Model.py	(refactored)
@@ -171,7 +171,7 @@
 
 #Simulate
 new_assignments = hlcm.predict(households.loc[hids], vacant_residential_units)
-print new_assignments
+print(new_assignments)
 
 
 # ## Employment Location Choice Model (ELCM)
@@ -225,7 +225,7 @@
 
 #Simulate
 new_assignments = elcm.predict(jobs.loc[jids], vacant_job_spaces)
-print new_assignments
+print(new_assignments)
 
 
 # ## Development Project Location Choice Model (DPLCM)
@@ -276,7 +276,7 @@
 
 #Simulate
 new_assignments = dplcm.predict(buildings.loc[bids], parcels.reset_index())
-print new_assignments
+print(new_assignments)
 
 
 # ## Real Estate Price Model (REPM)
@@ -332,7 +332,7 @@
 
 ##Simulate
 new_unit_price_res = hmg.predict(buildings)
-print new_unit_price_res
+print(new_unit_price_res)
 
 
 # ## Additional Examples (for illustration only)
RefactoringTool: Writing converted semcog_urbansim/notebooks/Location Choice Model.py to semcog_urbansim3/notebooks/Location Choice Model.py.
RefactoringTool: Can't parse semcog_urbansim/notebooks/Proforma Demo.py: ParseError: bad input: type=3, value="'unit_price_nonres > 0'", context=('                    ', (161, 21))
RefactoringTool: Refactored semcog_urbansim/notebooks/Proforma Demo.py
--- semcog_urbansim/notebooks/Proforma Demo.py	(original)
+++ semcog_urbansim/notebooks/Proforma Demo.py	(refactored)
@@ -1,282 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# import numpy as np, pandas as pd
-# from urbansim.models import MNLLocationChoiceModel, RegressionModelGroup, transition
-# from developer import developer, feasibility
-# from urbansim.urbansim import dataset
-# from variables import var_calc
-# 
-# dset = dataset.Dataset('data//semcog_data.h5')
-# vacant_residential_units, vacant_job_spaces = var_calc.calculate(dset)
-
-# ## Base year estimation 
-
-# In[2]:
-
-
-###HLCM
-#Agents for estimation
-households_for_estimation = dset.households.loc[np.random.choice(dset.households.index, size=15000, replace=False)]
-#Filters on agents
-agent_estimation_filters = ['building_id > 0']
-agents_simulation_filters = None
-##Specification
-patsy_expression = ['sqft_per_unit',
-                    'income:np.log1p(unit_price_res)',
-                    'jobs_within_30_min',
-                    'crime08',
-                    'popden']
-patsy_expression = ' + '.join(patsy_expression)
-#Filters on alternatives
-estimation_filters = ['building_id > 0']
-simulation_filters = ['residential_units>0',
-                      'building_type_id in [16,17,18,19,20]']
-interaction_filters = ['income*5 > unit_price_res']
-##Instantiate HCLM
-hlcm = MNLLocationChoiceModel(
-    patsy_expression, 10,
-    choosers_fit_filters=agent_estimation_filters, choosers_predict_filters=agents_simulation_filters,
-    alts_fit_filters=estimation_filters, alts_predict_filters=simulation_filters,interaction_predict_filters=interaction_filters,
-    choice_column='building_id', name='HLCM')
-##Estimate
-hlcm.fit(households_for_estimation, dset.buildings, households_for_estimation.building_id)
-hlcm.report_fit()
-
-
-###ELCM
-#Agents for estimation
-jobs_for_estimation = dset.jobs_for_estimation
-#Filters on agents
-agent_estimation_filters = ['home_based_status == 0']
-agents_simulation_filters = None
-##Specification
-patsy_expression = ['np.log1p(non_residential_sqft)',
-                    'np.log1p(improvement_value)',
-                    'jobs_within_30_min',
-                    'popden',
-                    'crime08']
-patsy_expression = ' + '.join(patsy_expression)
-#Filters on alternatives
-estimation_filters = ['building_id > 0']
-simulation_filters = ['non_residential_sqft>0',
-                      'building_type_id not in [16,17,18,19,20]']
-##Instantiate ELCM
-elcm = MNLLocationChoiceModel(
-    patsy_expression, 10,
-    choosers_fit_filters=agent_estimation_filters, choosers_predict_filters=agents_simulation_filters,
-    alts_fit_filters=estimation_filters, alts_predict_filters=simulation_filters,
-    choice_column='building_id', name='ELCM')
-##Estimate
-elcm.fit(jobs_for_estimation, dset.buildings, jobs_for_estimation.building_id)
-elcm.report_fit()
-
-# elcms = {}
-# for sector_id in dset.jobs_for_estimation[dset.jobs_for_estimation.home_based_status==0].groupby('sector_id').size().index.values:
-#     print 'SECTOR %s' % sector_id
-#     #Agents for estimation
-#     jobs_for_estimation = dset.jobs_for_estimation[dset.jobs_for_estimation.sector_id==sector_id]
-#     #Filters on agents
-#     agent_estimation_filters = ['home_based_status == 0']
-#     agents_simulation_filters = None
-#     ##Specification
-#     patsy_expression = ['np.log1p(non_residential_sqft)',
-#                         'np.log1p(improvement_value)',
-#                         'jobs_within_30_min',
-#                         'popden',
-#                         'crime08']
-#     patsy_expression = ' + '.join(patsy_expression)
-#     #Filters on alternatives
-#     estimation_filters = ['building_id > 0']
-#     simulation_filters = ['non_residential_sqft>0',
-#                           'building_type_id not in [16,17,18,19,20]']
-#     ##Instantiate ELCM
-#     elcm = MNLLocationChoiceModel(
-#         patsy_expression, 10,
-#         choosers_fit_filters=agent_estimation_filters, choosers_predict_filters=agents_simulation_filters,
-#         alts_fit_filters=estimation_filters, alts_predict_filters=simulation_filters,
-#         choice_column='building_id', name='ELCM')
-#     ##Estimate
-#     elcm.fit(jobs_for_estimation, dset.buildings, jobs_for_estimation.building_id)
-#     elcm.report_fit()
-#     ##Store sector-specific ELCMs
-#     elcms[sector_id] = elcm
-
-###Residential REPM
-##Model specification
-def patsy_expression():
-    patsy_exp = ['I(year_built < 1940)',
-                 'year_built',
-                 'stories',
-                 'np.log1p(sqft_per_unit)',
-                 'np.log1p(popden)',
-                 'dist_hwy',
-                 'dist_road',
-                 'crime08',
-                 'np.log1p(jobs_within_30_min)']
-    patsy_exp = ' + '.join(patsy_exp)
-    return 'np.log(unit_price_res) ~ ' + patsy_exp
-model_expression = patsy_expression()
-##Estimation filters
-estimate_filters = ['residential_units > 0',
-                    'sqft_per_unit > 0',
-                    'year_built > 1700',
-                    'stories > 0',
-                    'tax_exempt == 0',
-                    '1e5 < unit_price_res < 1e7',
-                    '16 <= building_type_id <= 20']
-##Simulation filters
-simulate_filters = ['residential_units > 0',
-                    '16 <= building_type_id <= 20']
-##Segmentation
-group_keys = [16, 17, 18, 19, 20]
-res_price_model = RegressionModelGroup('building_type_id')
-for key in group_keys:
-    res_price_model.add_model_from_params(key, estimate_filters, simulate_filters,
-                              model_expression, ytransform=np.exp)
-##Estimate
-fits_res = res_price_model.fit(dset.buildings)
-
-
-###Nonresidential REPM
-##Model specification
-def patsy_expression():
-    patsy_exp = ['I(year_built < 1940)',
-                 'year_built',
-                 'stories',
-                 'np.log1p(non_residential_sqft)',
-                 'np.log1p(popden)',
-                 'dist_hwy',
-                 'dist_road',
-                 'crime08',
-                 'np.log1p(jobs_within_30_min)']
-    patsy_exp = ' + '.join(patsy_exp)
-    return 'np.log(unit_price_nonres) ~ ' + patsy_exp
-model_expression = patsy_expression()
-##Estimation filters
-estimate_filters = ['non_residential_sqft > 0',
-                    'year_built > 1700',
-                    'stories > 0',
-                    'tax_exempt == 0',
-h                    'unit_price_nonres > 0',
-                    'building_type_id > 20']
-##Simulation filters
-simulate_filters = ['non_residential_sqft > 0',
-                    '16 <= building_type_id > 20']
-##Segmentation
-group_keys = [21,22,23,24,27,28,33,39] ##btype 32 had only 1 observation
-nonres_price_model = RegressionModelGroup('building_type_id')
-for key in group_keys:
-    nonres_price_model.add_model_from_params(key, estimate_filters, simulate_filters,
-                              model_expression, ytransform=np.exp)
-##Estimate
-fits_nonres = nonres_price_model.fit(dset.buildings)
-
-
-# ## Model Simulation
-
-# In[3]:
-
-
-for year in range (2011,2020):
-    
-    vacant_residential_units, vacant_job_spaces = var_calc.calculate(dset)
-    
-    ##Record county-level hh/job counts
-    dset.households['county_id'] = dset.buildings.county_id[dset.households.building_id].values
-    dset.jobs['county_id'] = dset.buildings.county_id[dset.jobs.building_id].values
-    starting_hh = dset.households.groupby('county_id').size()
-    starting_emp = dset.jobs.groupby('county_id').size()
-    
-    ##Household transition
-    ct = dset.fetch('annual_household_control_totals')
-    totals_field = ct.reset_index().groupby('year').total_number_of_households.sum()
-    ct = pd.DataFrame({'total_number_of_households':totals_field})
-    tran = transition.TabularTotalsTransition(ct, 'total_number_of_households')
-    model = transition.TransitionModel(tran)
-    new, added_hh_idx, new_linked = model.transition(
-            dset.households, year, linked_tables={'linked': (dset.persons, 'household_id')})
-    dset.households = new
-    dset.persons = new_linked['linked']
-    
-    ##Employment transition
-    ct_emp = dset.fetch('annual_employment_control_totals')
-    totals_field = ct_emp.reset_index().groupby('year').total_number_of_jobs.sum()
-    ct_emp = pd.DataFrame({'total_number_of_jobs':totals_field})
-    tran = transition.TabularTotalsTransition(ct_emp, 'total_number_of_jobs')
-    model = transition.TransitionModel(tran)
-    new, added_jobs_idx, new_linked = model.transition(dset.jobs, year)
-    dset.jobs = new
-    
-    ###HLCM
-    new_assignments = hlcm.predict(dset.households.loc[added_hh_idx], vacant_residential_units)
-    dset.households.loc[added_hh_idx,'building_id'] = new_assignments
-    
-    ###ELCM
-    new_assignments = elcm.predict(dset.jobs.loc[added_jobs_idx], vacant_job_spaces)
-    dset.jobs.loc[added_jobs_idx,'building_id'] = new_assignments
-#     job_sectors = dset.jobs.loc[added_jobs_idx, 'sector_id']
-#     for sector in np.unique(job_sectors.values):
-#         if sector in elcms.keys():
-#             print sector
-#             idx_by_sector = job_sectors[job_sectors.values==sector]
-#             added_jobs_idx_by_sector = idx_by_sector.index
-#             elcm = elcms[sector]
-#             print elcm
-#             new_assignments = elcm.predict(dset.jobs.loc[added_jobs_idx_by_sector], vacant_job_spaces)  ##vacant job spaces needs to be updated
-#             dset.jobs.loc[added_jobs_idx_by_sector,'building_id'] = new_assignments
-    
-    ####REPM
-    new_unit_price_res = res_price_model.predict(dset.buildings)
-    dset.buildings.loc[new_unit_price_res.index.values, 'unit_price_res'] = new_unit_price_res
-    
-    new_unit_price_nonres = nonres_price_model.predict(dset.buildings)
-    dset.buildings.loc[new_unit_price_nonres.index.values, 'unit_price_nonres'] = new_unit_price_nonres
-    
-    ##County-level hh/job growth indicator
-    dset.households['county_id'] = dset.buildings.county_id[dset.households.building_id].values
-    dset.jobs['county_id'] = dset.buildings.county_id[dset.jobs.building_id].values
-    ending_hh = dset.households.groupby('county_id').size()
-    ending_emp = dset.jobs.groupby('county_id').size()
-    print 'HH diffs'
-    print ending_hh - starting_hh
-    print 'EMP diffs'
-    print ending_emp - starting_emp
-    
-    
-    ####PROFORMA
-    feasibility.feasibility_run(dset,year)
-    
-    ##Record starting non-residential sqft by county
-    dset.buildings['county_id'] = dset.parcels.county_id[dset.buildings.parcel_id].values
-    starting_nonres = dset.buildings.groupby('county_id').non_residential_sqft.sum()
-    
-    ##Non-residential proformas
-    nr_buildings = developer.exec_developer(dset, year, "jobs", "job_spaces",
-                   [21,22,23,24,27,28,32,33,39], nonres=True)
-    dset.buildings = dset.buildings[['building_id_old', 'building_type_id', 'improvement_value', 'land_area',
-         'non_residential_sqft', 'parcel_id', 'residential_units', 'sqft_per_unit',
-         'stories', 'tax_exempt', 'year_built']]
-    dset.buildings = pd.concat([dset.buildings,nr_buildings])
-    
-    ##County-level non-residential-sqft growth indicator
-    dset.buildings['county_id'] = dset.parcels.county_id[dset.buildings.parcel_id].values
-    ending_nonres = dset.buildings.groupby('county_id').non_residential_sqft.sum()
-    print 'Nonres sqft diffs'
-    print ending_nonres - starting_nonres
-    
-    ##Record starting residential units by county
-    dset.buildings['county_id'] = dset.parcels.county_id[dset.buildings.parcel_id].values
-    starting_resunits = dset.buildings.groupby('county_id').residential_units.sum()
-    
-    ##Residential proformas
-    res_buildings = developer.exec_developer(dset, year, "households", 
-                                             "residential_units", [16, 17, 18, 19])
-    dset.buildings = pd.concat([dset.buildings,res_buildings])
-    
-    ##County-level residential unit growth indicator
-    dset.buildings['county_id'] = dset.parcels.county_id[dset.buildings.parcel_id].values
-    ending_resunits = dset.buildings.groupby('county_id').residential_units.sum()
-    print 'Resunit diffs'
-    print ending_resunits - starting_resunits
-
+Non
RefactoringTool: Writing converted semcog_urbansim/notebooks/Proforma Demo.py to semcog_urbansim3/notebooks/Proforma Demo.py.
RefactoringTool: No changes to semcog_urbansim/notebooks/Residential Price Model.py
RefactoringTool: Writing converted semcog_urbansim/notebooks/Residential Price Model.py to semcog_urbansim3/notebooks/Residential Price Model.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/accessibility_w_local.py
--- semcog_urbansim/notebooks/accessibility_w_local.py	(original)
+++ semcog_urbansim/notebooks/accessibility_w_local.py	(refactored)
@@ -63,7 +63,7 @@
 t1=time.time()
 x, y = parcels.centroid_x, parcels.centroid_y
 parcels["node_ids"] = net_local.get_node_ids(x, y)
-print time.time()-t1
+print(time.time()-t1)
 
 
 # In[14]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/accessibility_w_local.py to semcog_urbansim3/notebooks/accessibility_w_local.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/refine by changing hh pop.py
--- semcog_urbansim/notebooks/refine by changing hh pop.py	(original)
+++ semcog_urbansim/notebooks/refine by changing hh pop.py	(refactored)
@@ -53,7 +53,7 @@
 
 
 for ind in refinement[str(2020)].index:
-    print ind
+    print(ind)
 
 
 # In[ ]:
@@ -73,11 +73,11 @@
         amount = refinement.loc[city_id, year]
         pre = len(dfpc)
         
-        print city_id, 'before: ', pre,  'add:', amount
+        print(city_id, 'before: ', pre,  'add:', amount)
         
         if amount >0:
             p_add = dfpc.sample(amount)
-            p_add.index=range(dfp.index.max()+1, dfp.index.max()+1+len(p_add))
+            p_add.index=list(range(dfp.index.max()+1, dfp.index.max()+1+len(p_add)))
             dfp = pd.concat([dfp,p_add])
         elif amount<0:
             p_remove = dfpc.sample(amount).index
@@ -88,7 +88,7 @@
     dfh['persons'] = dfp.groupby('household_id').size()
     dfh['workers'] = dfp.groupby('household_id').worker.sum()
     dfh.fillna(0, inplace=True)
-    print 'after:', len(dfp), dfh.persons.sum()
+    print('after:', len(dfp), dfh.persons.sum())
     
     st['/' + year + '/households'] = dfh
     st['/' + year + '/persons'] =  dfp
@@ -222,7 +222,7 @@
 # In[ ]:
 
 
-1633176426+range(10)
+1633176426+list(range(10))
 
 
 # In[ ]:
@@ -241,7 +241,7 @@
 
 
 for year in refinement.columns.sort_values().unique():
-    print "year", year
+    print("year", year)
     b_city_id = hdf[str(year) + '/buildings'][['b_city_id']]
     households = hdf[str(year) + '/households']
     households_col = households.columns
@@ -260,7 +260,7 @@
         
         sample_size = min(10 * abs(target), len(hh_main), len(hh_other))
         
-        print year, main_city, target, len(hh_main), len(hh_other), sample_size
+        print(year, main_city, target, len(hh_main), len(hh_other), sample_size)
 
         hh_main_sample = hh_main.sample(sample_size)
         hh_other_sample = hh_other.sample(sample_size)
@@ -287,7 +287,7 @@
         ref = (gole - post).astype(int)
     
     hdf[str(year) + '/households'] = households[households_col]
-    print "done"
+    print("done")
 
 
 # In[ ]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/refine by changing hh pop.py to semcog_urbansim3/notebooks/refine by changing hh pop.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/refine by hh swap.py
--- semcog_urbansim/notebooks/refine by hh swap.py	(original)
+++ semcog_urbansim/notebooks/refine by hh swap.py	(refactored)
@@ -47,7 +47,7 @@
 
 
 for year in refinement.columns.sort_values().unique():
-    print "year", year
+    print("year", year)
     b_city_id = hdf[str(year) + '/buildings'][['b_city_id']]
     households = hdf[str(year) + '/households']
     households_col = households.columns
@@ -66,7 +66,7 @@
         
         sample_size = min(10 * abs(target), len(hh_main), len(hh_other))
         
-        print year, main_city, target, len(hh_main), len(hh_other), sample_size
+        print(year, main_city, target, len(hh_main), len(hh_other), sample_size)
 
         hh_main_sample = hh_main.sample(sample_size)
         hh_other_sample = hh_other.sample(sample_size)
@@ -93,7 +93,7 @@
         ref = (gole - post).astype(int)
     
     hdf[str(year) + '/households'] = households[households_col]
-    print "done"
+    print("done")
 
 
 # In[10]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/refine by hh swap.py to semcog_urbansim3/notebooks/refine by hh swap.py.
RefactoringTool: No changes to semcog_urbansim/notebooks/shape2hdf5_w_local.py
RefactoringTool: Writing converted semcog_urbansim/notebooks/shape2hdf5_w_local.py to semcog_urbansim3/notebooks/shape2hdf5_w_local.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/shape2net.py
--- semcog_urbansim/notebooks/shape2net.py	(original)
+++ semcog_urbansim/notebooks/shape2net.py	(refactored)
@@ -86,7 +86,7 @@
 # In[13]:
 
 
-print dicpkl['edgeids'].size, dicpkl['edges'].size, dicpkl['edgeweights'].size, dicpkl['nodeids'].size, dicpkl['nodes'].size
+print(dicpkl['edgeids'].size, dicpkl['edges'].size, dicpkl['edgeweights'].size, dicpkl['nodeids'].size, dicpkl['nodes'].size)
 
 
 # In[14]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/shape2net.py to semcog_urbansim3/notebooks/shape2net.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/shape2net_w_localnodes.py
--- semcog_urbansim/notebooks/shape2net_w_localnodes.py	(original)
+++ semcog_urbansim/notebooks/shape2net_w_localnodes.py	(refactored)
@@ -78,7 +78,7 @@
 dicpkl['nodeids']=df_nodes[i].index.values.astype('int32')
 dicpkl['nodes']=df_nodes[i][['x','y']].values.astype('float32')
 
-print 'pickle'+str(i), [(key, dicpkl[key].size) for key in dicpkl.keys()]
+print('pickle'+str(i), [(key, dicpkl[key].size) for key in list(dicpkl.keys())])
 
 #save to pickle 
 with open(fn_pkls[i], 'wb') as handle:
RefactoringTool: Writing converted semcog_urbansim/notebooks/shape2net_w_localnodes.py to semcog_urbansim3/notebooks/shape2net_w_localnodes.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/shape2pickle.py
--- semcog_urbansim/notebooks/shape2pickle.py	(original)
+++ semcog_urbansim/notebooks/shape2pickle.py	(refactored)
@@ -86,7 +86,7 @@
 # In[13]:
 
 
-print dicpkl['edgeids'].size, dicpkl['edges'].size, dicpkl['edgeweights'].size, dicpkl['nodeids'].size, dicpkl['nodes'].size
+print(dicpkl['edgeids'].size, dicpkl['edges'].size, dicpkl['edgeweights'].size, dicpkl['nodeids'].size, dicpkl['nodes'].size)
 
 
 # In[14]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/shape2pickle.py to semcog_urbansim3/notebooks/shape2pickle.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/model_analysis/ELCM_Diagnostics.py
--- semcog_urbansim/notebooks/model_analysis/ELCM_Diagnostics.py	(original)
+++ semcog_urbansim/notebooks/model_analysis/ELCM_Diagnostics.py	(refactored)
@@ -158,7 +158,7 @@
 
 
 for large_area_id in large_area_ids:
-    print large_area_id
+    print(large_area_id)
     w = get_submodel_coeffs(large_area_id)
     proba = np.transpose(calculate_proba(w, x))
 
@@ -166,7 +166,7 @@
     proba_df['building_type_id'] = buildings.building_type_id.values
 
     summed_proba_btype = proba_df.groupby('building_type_id').sum()
-    print summed_proba_btype
+    print(summed_proba_btype)
 
 
 # ## Summed capacity-weighted probabilities by building type
@@ -224,7 +224,7 @@
 
 
 for large_area_id in large_area_ids:
-    print large_area_id
+    print(large_area_id)
     w = get_submodel_coeffs(large_area_id)
     capac_weighted_proba = np.transpose(capacity_weighted_proba(w, x, vacant_job_spaces))
 
@@ -232,8 +232,8 @@
     proba_df['building_type_id'] = buildings.building_type_id.values
 
     summed_capac_weighted_proba_btype = proba_df.groupby('building_type_id').sum()
-    print summed_capac_weighted_proba_btype
-    print''
+    print(summed_capac_weighted_proba_btype)
+    print('')
 
 
 # ## What building type dummy coefficient value may result in closer fit between simulated / observed sector shares by building type
@@ -287,16 +287,16 @@
 
 
 for large_area_id in large_area_ids:
-    print large_area_id
+    print(large_area_id)
     w = get_submodel_coeffs(large_area_id)
 
     result = opt.minimize_scalar(growth_share_deviation, method='Brent', args=(w, x, sectoral_changes, idx_medical, .44, -1, -3, vacant_job_spaces))
-    print result.x
+    print(result.x)
 
     result = opt.minimize_scalar(growth_share_deviation, method='Brent', args=(w, x, sectoral_changes, idx_tcu, .331, 3, -1, vacant_job_spaces))
-    print result.x
-
-    print ''
+    print(result.x)
+
+    print('')
 
 
 # ## Suggested next steps
RefactoringTool: Writing converted semcog_urbansim/notebooks/model_analysis/ELCM_Diagnostics.py to semcog_urbansim3/notebooks/model_analysis/ELCM_Diagnostics.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py
--- semcog_urbansim/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py	(original)
+++ semcog_urbansim/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py	(refactored)
@@ -60,7 +60,7 @@
 
 
 expvars = []
-for submodel_id in model_coeffs.keys():
+for submodel_id in list(model_coeffs.keys()):
     expvars.extend(model_coeffs[submodel_id].index)
     
 expvars = list(set(expvars))
@@ -357,21 +357,21 @@
 
 
 result = opt.minimize_scalar(growth_share_deviation, method='Brent', args=(w, x, quartile_changes, idx_mode_1, .09, 3, 18, vacant_spaces))
-print result.x
+print(result.x)
 
 
 # In[41]:
 
 
 result = opt.minimize_scalar(growth_share_deviation, method='Brent', args=(w, x, quartile_changes, idx_mode_1, .19, 2, 18, vacant_spaces))
-print result.x
+print(result.x)
 
 
 # In[42]:
 
 
 result = opt.minimize_scalar(growth_share_deviation, method='Brent', args=(w, x, quartile_changes, idx_mode_1, .31, 1, 18, vacant_spaces))
-print result.x
+print(result.x)
 
 
 # In[ ]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py to semcog_urbansim3/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py
--- semcog_urbansim/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py(original)
+++ semcog_urbansim/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py(refactored)
@@ -46,16 +46,16 @@
 
 model_configs = lcm_utils.get_model_category_configs()
 
-for model_category_name, model_category_attributes in model_configs.items():
+for model_category_name, model_category_attributes in list(model_configs.items()):
     if model_category_name == 'elcm':
         for yaml_config in model_category_attributes['config_filenames']:
-            print yaml_config
+            print(yaml_config)
             model = MNLDiscreteChoiceModel.from_yaml(str_or_buffer=misc.config(yaml_config))
             
             # Patsy-form specification
             patsy_str = ' + '
             patsy_str = patsy_str.join(model.model_expression) + ' - 1'
-            print patsy_str
+            print(patsy_str)
             
             # Pylogit-form specification
             vars_for_dict = OrderedDict([(varname, 'all_same') for varname in model.model_expression])
@@ -81,9 +81,9 @@
                          choice_col = data.choice_col,
                          model_expression = vars_for_dict,
                          alternative_id_col='building_id') #patsy_str
-            print model._estimation_engine
+            print(model._estimation_engine)
             
             results = model.fit()
-            print results
-            print ''
+            print(results)
+            print('')
 
RefactoringTool: Writing converted semcog_urbansim/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py to semcog_urbansim3/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/ELCM Estimation.py
--- semcog_urbansim/notebooks/others/ELCM Estimation.py	(original)
+++ semcog_urbansim/notebooks/others/ELCM Estimation.py	(refactored)
@@ -33,7 +33,7 @@
     j = orca.get_table('jobs').to_frame(model.columns_used()).fillna(0)
     b = orca.get_table('buildings').to_frame(model.columns_used()).fillna(0)
 
-    print model.fit(j, b, j[model.choice_column])
+    print(model.fit(j, b, j[model.choice_column]))
 
     return model.fit_parameters
 
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/ELCM Estimation.py to semcog_urbansim3/notebooks/others/ELCM Estimation.py.
RefactoringTool: Can't parse semcog_urbansim/notebooks/others/ELCM_variables.py: ParseError: bad input: type=4, value='\n', context=('', (204, 99))
RefactoringTool: Refactored semcog_urbansim/notebooks/others/ELCM_variables.py
--- semcog_urbansim/notebooks/others/ELCM_variables.py	(original)
+++ semcog_urbansim/notebooks/others/ELCM_variables.py	(refactored)
@@ -1,299 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# In[ ]:
-
-
-import pandas as pd
-import numpy as np
-import time
-import os
-import random
-#from urbansim.models import transition, relocation
-from urbansim.developer import sqftproforma, developer
-from urbansim.utils import misc, networks
-import dataset, variables, utils
-import pandana as pdna
-import models
-
-import orca
-
-
-# In[ ]:
-
-
-bb =orca.get_table('buildings').to_frame()
-
-
-# In[ ]:
-
-
-len(bb.columns.values)
-
-
-# In[ ]:
-
-
-import matplotlib.pyplot as plt
-get_ipython().run_line_magic('matplotlib', 'inline')
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-import numpy as np
-
-
-# In[ ]:
-
-
-bbres= bb[bb.building_type_id.isin([81,82,83,84])]
-
-
-# In[ ]:
-
-
-bbres = bbres[bbres.sqft_price_res>20]
-
-
-# In[ ]:
-
-
-pd.qcut(bbres.sqft_price_res,20)
-
-
-# In[ ]:
-
-
-bbres.sqft_price_res.hist()
-
-
-# In[ ]:
-
-
-bbres.shape
-
-
-# In[ ]:
-
-
-bb.to_csv('buildings_fullv.csv')
-
-
-# In[ ]:
-
-
-orca.run(["build_networks"])
-orca.run(["neighborhood_vars"])
-
-
-# In[ ]:
-
-
-orca.list_tables()
-
-
-# # Variables converted from OPUS(urbansim1) to ORCA(urbansim2)
-
-# In[ ]:
-
-
-#building type dummy variables (derived from building types table,we have 27 building types)
-df=pd.get_dummies(orca.merge_tables(target='buildings', tables=['building_types', 'buildings'], 
-                            columns=["building_type_name "])["building_type_name "]).astype(bool)
-df.columns = ["type_is_" + i.strip().replace(" ",'_') for i in df.columns]
-
-
-
-B_ln_nonres_sqft = np.log( orca.get_table('buildings').to_frame('non_residential_sqft')['non_residential_sqft'] )
-B_ln_sqft = np.log( orca.get_table('buildings').to_frame('building_sqft')['building_sqft'] )
-B_ln_land_area = np.log( orca.get_table('buildings').to_frame('land_area')['land_area'] )
-B_ln_lot_sqft = np.log(orca.get_table('buildings').to_frame('parcel_sqft')['parcel_sqft'])
-B_ln_sqft_unit = np.log( orca.get_table('buildings').to_frame('sqft_per_unit')['sqft_per_unit'] ) 
-B_building_age = 2015 - orca.get_table('buildings').to_frame('year_built')['year_built']  #in simulation , use iter_var instead of 2015
-B_far=orca.merge_tables(target='buildings', tables=['parcels', 'buildings'], 
-                            columns=["parcel_far"])["parcel_far"]
-B_ln_invfar = -np.log(B_far)
-B_is_pre_1945 = orca.get_table('buildings').to_frame('year_built')['year_built'] < 1945,
-B_is_newerthan2010 = orca.get_table('buildings').to_frame('year_built')['year_built'] > 2010,
-B_sqft_price = orca.get_table('buildings').to_frame('sqft_price_nonres')['sqft_price_nonres']
-B_ln_sqft_price = np.log( B_sqft_price )
-
-Z_jobs=orca.get_table('zones').to_frame('employment')['employment']
-Z_households=orca.get_table('zones').to_frame('households')['households']
-Z_ln_empden = np.log(orca.get_table('zones').to_frame('empden')['empden'])
-Z_ln_popden = np.log(orca.get_table('zones').to_frame('popden')['popden'])
-
-
-N_walk_quarter_mile_retail = orca.merge_tables(target='buildings', tables=['nodes_walk', 'buildings'], 
-                            columns=["retail_jobs"])["retail_jobs"]
-N_ln_avginc = orca.merge_tables(target='buildings', tables=['nodes_walk', 'buildings'], 
-                            columns=["ave_income"])["ave_income"]
-N_nonres_ave_price = orca.merge_tables(target='buildings', tables=['nodes_walk', 'buildings'], 
-                            columns=["ave_nonres_sqft_price"])["ave_nonres_sqft_price"]
-
-# within quarter mile to transit stops
-N_close_to_transit = orca.merge_tables(target='buildings', tables=['nodes_walk', 'buildings'], 
-                            columns=["quarter_mile_to_transit"])["quarter_mile_to_transit"]
-
-
-
-##########################################
-##### updated variables #####
-
-
-#---- old variables from Feburary -----#
-
-A_job_logsum_worker_ge_car=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_work_more_worker_than_car"])["logsum_work_more_worker_than_car"]
-A_job_logsum_worker_lt_car=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_work_less_worker_than_car"])["logsum_work_less_worker_than_car"]
-A_pop_logsum_worker_ge_car=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_more_worker_than_car"])["logsum_pop_more_worker_than_car"]
-A_pop_logsum_worker_lt_car=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_less_worker_than_car"])["logsum_pop_less_worker_than_car"]
-#transit jobs within 45 minutes based on travel model zone-to-zone transit total time
-td = orca.get_table('travel_data').to_frame('am_total_transit_time_walk').reset_index()
-zemp = orca.get_table('zones').to_frame('employment')
-temp = pd.merge(td,zemp, left_on = 'to_zone_id', right_index = True, how='left' )
-A_transit_jobs_45min = temp[temp.am_total_transit_time_walk <=45].groupby('from_zone_id').employment.sum()
-
-
-
-
-
-#---- new definition by March 22nd-----#
-A_job_logsum_high_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_high_income"])["logsum_pop_high_income"]
-A_job_logsum_low_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_low_income"])["logsum_pop_low_income"]
-A_pop_logsum_high_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_high_income"])["logsum_pop_high_income"]
-A_pop_logsum_low_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_low_income"])["logsum_pop_low_income"]
-#transit jobs within 50 minutes based on travel model zone-to-zone transit total time
-@orca.column('zones')
-def A_ln_emp_50min_transit(zones, travel_data):
-    transittime = travel_data.to_frame("am_transit_total_time").reset_index()
-    zemp = zones.to_frame('employment')
-    temp = pd.merge(transittime,zemp, left_on = 'to_zone_id', right_index = True, how='left' )
-    return np.log1p(temp[temp.am_transit_total_time <=50].groupby('from_zone_id').employment.sum().fillna(0))
-
-
-# # Variables haven't been converted from OPUS(urbansim1) to ORCA(urbansim2)
-
-# In[ ]:
-
-
-#description: from OPUS(urbansim1) job_X_building interaction, summarization on number of jobs in the buildings that are in the same sector as the new jobs (chooser)
-B_same_sector_emp_in_bldg = urbansim_parcel.job_x_building.same_sector_employment_in_building,
-    
-#description: from OPUS job_X_building,the ratio of same sector jobs vs total jobs in the building    
-B_sector_density_in_building = (urbansim_parcel.job_x_building.same_sector_employment_in_building)/
-    (urbansim_parcel.building.number_of_non_home_based_jobs),
-     
-# number of jobs by sector vs total jobs at building level. total 18 sectors
-B_bldg_empratio_1 = urbansim_parcel.building.number_of_jobs_of_sector_1/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_2 = urbansim_parcel.building.number_of_jobs_of_sector_2/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_3 = urbansim_parcel.building.number_of_jobs_of_sector_3/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_4 = urbansim_parcel.building.number_of_jobs_of_sector_4/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_5 = urbansim_parcel.building.number_of_jobs_of_sector_5/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_6 = urbansim_parcel.building.number_of_jobs_of_sector_6/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_7 = urbansim_parcel.building.number_of_jobs_of_sector_7/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_8 = urbansim_parcel.building.number_of_jobs_of_sector_8/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_9 = urbansim_parcel.building.number_of_jobs_of_sector_9/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_10 = urbansim_parcel.building.number_of_jobs_of_sector_10/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_11 = urbansim_parcel.building.number_of_jobs_of_sector_11/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_12 = urbansim_parcel.building.number_of_jobs_of_sector_12/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_13 = urbansim_parcel.building.number_of_jobs_of_sector_13/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_14 = urbansim_parcel.building.number_of_jobs_of_sector_14/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_15 = urbansim_parcel.building.number_of_jobs_of_sector_15/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_16 = urbansim_parcel.building.number_of_jobs_of_sector_16/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_17 = urbansim_parcel.building.number_of_jobs_of_sector_17/urbansim_parcel.building.number_of_jobs
-B_bldg_empratio_18 = urbansim_parcel.building.number_of_jobs_of_sector_18/urbansim_parcel.building.number_of_jobs
-
-
-# zonal employment density by sector. this could be replaced by employment accessibility indicators
-Z_ln_zone_empden_1 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_1)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_2 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_2)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_3 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_3)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_4 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_4)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_5 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_5)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_6 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_6)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_7 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_7)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_8 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_8)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_9 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_9)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_10 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_10)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_11 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_11)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_12 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_12)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_13 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_13)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_14 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_14)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_15 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_15)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_16 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_16)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_17 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_17)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-Z_ln_zone_empden_18 = ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_18)
-                      /building.disaggregate(zone.aggregate(parcel.parcel_sqft)/43560.0))
-
-
-# # variables for future test
-
-# In[ ]:
-
-
-======================================================================
-
-
-# N_auto_30min_jobs = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_30min_jobs"])["drv_30min_jobs"]
-# N_auto_45min_jobs = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_45min_jobs"])["drv_45min_jobs"]
-# N_auto_60min_jobs = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_60min_jobs"])["drv_60min_jobs"]
-# N_auto_10min_pop = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_30min_jobs"])["drv_30min_jobs"]
-# N_auto_20min_pop = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_45min_jobs"])["drv_45min_jobs"]
-# N_auto_10min_retail = orca.merge_tables(target='buildings', tables=['nodes_drv','buildings'], 
-#                             columns=["drv_10min_retail_jobs"])["drv_10min_retail_jobs"]
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-
-
+Non
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/ELCM_variables.py to semcog_urbansim3/notebooks/others/ELCM_variables.py.
RefactoringTool: No changes to semcog_urbansim/notebooks/others/Exploration.py
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/Exploration.py to semcog_urbansim3/notebooks/others/Exploration.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/HLCM Estimation.py
--- semcog_urbansim/notebooks/others/HLCM Estimation.py	(original)
+++ semcog_urbansim/notebooks/others/HLCM Estimation.py	(refactored)
@@ -33,7 +33,7 @@
     hh = orca.get_table('households').to_frame(model.columns_used()).fillna(0)
     b = orca.get_table('buildings').to_frame(model.columns_used()).fillna(0)
 
-    print model.fit(hh, b, hh[model.choice_column])
+    print(model.fit(hh, b, hh[model.choice_column]))
 
     return model.fit_parameters
 
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/HLCM Estimation.py to semcog_urbansim3/notebooks/others/HLCM Estimation.py.
RefactoringTool: Can't parse semcog_urbansim/notebooks/others/HLCM_variables.py: ParseError: bad input: type=5, value=' ', context=("\n\n# # Variables haven't been converted from OPUS(urbansim1) to ORCA(urbansim2)\n\n# In[ ]:\n\n\n## income\n", (289, 0))
RefactoringTool: Refactored semcog_urbansim/notebooks/others/HLCM_variables.py
--- semcog_urbansim/notebooks/others/HLCM_variables.py	(original)
+++ semcog_urbansim/notebooks/others/HLCM_variables.py	(refactored)
@@ -1,358 +1 @@
-#!/usr/bin/env python
-# coding: utf-8
-
-# In[1]:
-
-
-import pandas as pd
-import numpy as np
-import time
-import os
-import random
-#from urbansim.models import transition, relocation
-from urbansim.developer import sqftproforma, developer
-from urbansim.utils import misc, networks
-#import dataset, variables, utils, transcad
-import dataset, variables, utils
-import pandana as pdna
-import models
-
-import orca
-
-
-# In[2]:
-
-
-dfh=orca.get_table('households').to_frame(orca.get_table('households').local_columns + ['large_area_id'])
-
-
-# In[4]:
-
-
-dfh.to_csv('syn_households.csv')
-
-
-# In[5]:
-
-
-dfp=orca.get_table('persons').to_frame(orca.get_table('persons').local_columns + ['large_area_id'])
-
-
-# In[6]:
-
-
-dfp.to_csv('syn_persons.csv')
-
-
-# In[2]:
-
-
-orca.list_tables()
-
-
-# In[4]:
-
-
-orca.get_table('target_vacancies').to_frame()
-
-
-# In[ ]:
-
-
-
-
-
-# In[ ]:
-
-
-
-
-
-# In[54]:
-
-
-sjb = dfj.groupby('building_id').size()
-
-
-# In[55]:
-
-
-sjb.name='jobs'
-
-
-# In[62]:
-
-
-b4jobs = pd.merge(dfb, sjb.to_frame(), left_index=True, right_index=True, how='left').fillna(0)
-
-
-# In[63]:
-
-
-b4jobs.to_csv('b4jobs.csv')
-
-
-# In[44]:
-
-
-orca.list_tables()
-
-
-# In[64]:
-
-
-orca.get_table('parcels').to_frame(orca.get_table('parcels').local_columns)
-
-
-# In[39]:
-
-
-dd=dfh.reset_index()
-
-
-# In[40]:
-
-
-dd.loc[dd.large_area_id>100, ['household_id','large_area_id']]
-
-
-# In[15]:
-
-
-dfp=orca.get_table('persons').to_frame()[['household_id','age','sex','race_id']]
-
-
-# In[18]:
-
-
-pd.merge(dfp,dfh, left_on='household_id', right_index=True, how='left').to_csv('syn_persons_4control.csv')
-
-
-# In[31]:
-
-
-for ld, hh in dfh.groupby('large_area_id'):
-    print hh.head(20)
-    print '--'
-
-
-# In[ ]:
-
-
-orca.run(["build_networks"])
-orca.run(["neighborhood_vars"])
-
-
-# In[ ]:
-
-
-orca.get_table('households').to_frame()
-
-
-# In[ ]:
-
-
-
-
-
-# # Variables converted from OPUS(urbansim1) to ORCA(urbansim2)
-
-# In[ ]:
-
-
-
-#building type dummy variables (derived from building types table,we have 27 building types)
-df=pd.get_dummies(orca.merge_tables(target='buildings', tables=['building_types', 'buildings'], 
-                            columns=["building_type_name "])["building_type_name "]).astype(bool)
-df.columns = ["type_is_" + i.strip().replace(" ",'_') for i in df.columns]
-
-
-B_building_age = 2015 - orca.get_table('buildings')['year_built']  #in simulation , use iter_var instead of 2015
-B_is_new_construction = B_building_age <= 2
-B_is_pre1950 = orca.get_table('buildings')['year_built'] < 1950
-B_ln_sqft_per_unit=np.log1p(orca.get_table('buildings')['sqft_per_unit']) #filter out 0s?
-
-@orca.column('buildings')
-def lot_sqft_per_unit(parcels, buildings):
-    b = buildings.to_frame(["parcel_id", "residential_units"])
-    rh_per_p = b.groupby("parcel_id").residential_units.sum()
-    sqft_per_rh = parcels.to_frame("parcel_sqft").parcel_sqft / rh_per_p
-    out = misc.reindex(sqft_per_rh, b.parcel_id)
-    return out.fillna(1).replace(np.inf, 1)
-
-
-B_ln_lot_size_per_unit = np.log(orca.get_table('buildings')['lot_sqft_per_unit'])
-B_ln_price_per_unit = np.log(orca.get_table('buildings')['sqft_price_res'])
-B_ln_price_per_sqft = np.log1p((orca.get_table('buildings')['sqft_price_res']/orca.get_table('buildings')['sqft_per_unit']))
-B_ln_residential_units = np.log(orca.get_table('buildings')['residential_units'])
-B_ln_vacant_residential_units = np.log(orca.get_table('buildings')['vacant_residential_units'])
-
-B_far=orca.merge_tables(target='buildings', tables=['parcels', 'buildings'], columns=["parcel_far"])["parcel_far"]
-B_ln_invfar = -np.log(B_far)
-B_school_district_achievement = orca.get_table('buildings')['school_district_achievement']
-
-
-# segment or dumy by large area
-
-Z_jobs = orca.get_table('zones')['employment']
-Z_households = orca.get_table('zones')['households']
-Z_ln_households = np.log(Z_households)
-Z_population =orca.get_table('zones')['population']
-Z_ln_population = np.log(Z_population)
-Z_ln_empden = np.log(orca.get_table('zones')['empden'])
-Z_ln_popden = np.log(orca.get_table('zones')['popden'])
-
-## To be ramade as pandana baset n variables
-N_ln_avginc = orca.merge_tables(target='buildings', tables=['nodes_walk', 'buildings'], 
-                             columns=["ave_income"])["ave_income"]
-
-N_households = orca.get_table('nodes_walk')['households']
-N_ln_households = np.log(N_households)
-N_population = orca.get_table('nodes_walk')['population']
-N_ln_population = np.log(N_population)
-N_jobs = orca.get_table('nodes_walk')['jobs']
-node_r1500_acre = orca.get_table('nodes_walk')['node_r1500_sqft'] / 43560.
-
-N_ln_empden = np.log1p(N_jobs / node_r1500_acre)
-N_ln_popden = np.log1p(N_population / node_r1500_acre)
-
-N_percent_high_income = orca.get_table('nodes_walk')['highinc_hhs'] / N_households
-N_percent_mid_income = orca.get_table('nodes_walk')['midinc_hhs'] / N_households
-N_percent_low_income = orca.get_table('nodes_walk')['lowinc_hhs'] / N_households
-
-N_percent_race1=N_percent_mid_income = orca.get_table('nodes_walk')['race_1_hhs'] / N_households
-N_percent_race2=N_percent_mid_income = orca.get_table('nodes_walk')['race_2_hhs'] / N_households
-N_percent_race3=N_percent_mid_income = orca.get_table('nodes_walk')['race_3_hhs'] / N_households
-N_percent_race4=N_percent_mid_income = orca.get_table('nodes_walk')['race_4_hhs'] / N_households
-
-N_percent_hh_with_children = orca.get_table('nodes_walk')['hhs_with_children'] / N_households
-
-N_ln_average_zonal_income = np.log(orca.get_table('nodes_walk')['ave_income'])
-
-# dumy variabal per mcd
-
-C_crime_ucr = orca.get_table('parcels')['crime_ucr_rate']
-C_crime_other = orca.get_table('parcels')['crime_other_rate']
-
-#accessibility based on zone-to-zone travel matrix
-@orca.column('zones')
-def A_ln_emp_26min_drive_alone(zones, travel_data):
-    drvtime = travel_data.to_frame("am_auto_total_time").reset_index()
-    zemp = zones.to_frame('employment')
-    temp = pd.merge(drvtime,zemp, left_on = 'to_zone_id', right_index = True, how='left' )
-    return np.log1p(temp[temp.am_auto_total_time <=26].groupby('from_zone_id').employment.sum().fillna(0))
-
-@orca.column('zones')
-def A_ln_emp_50min_transit(zones, travel_data):
-    transittime = travel_data.to_frame("am_transit_total_time").reset_index()
-    zemp = zones.to_frame('employment')
-    temp = pd.merge(transittime,zemp, left_on = 'to_zone_id', right_index = True, how='left' )
-    return np.log1p(temp[temp.am_transit_total_time <=50].groupby('from_zone_id').employment.sum().fillna(0))
-
-@orca.column('zones')
-def A_ln_retail_emp_15min_drive_alone(zones, travel_data):
-    drvtime = travel_data.to_frame("midday_auto_total_time").reset_index()
-    zemp = zones.to_frame('employment')
-    temp = pd.merge(drvtime,zemp, left_on = 'to_zone_id', right_index = True, how='left' )
-    return np.log1p(temp[temp.midday_auto_total_time <=15].groupby('from_zone_id').employment.sum().fillna(0))
-
-
-A_job_logsum_high_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_high_income"])["logsum_pop_high_income"]
-A_job_logsum_low_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_low_income"])["logsum_pop_low_income"]
-A_pop_logsum_high_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_high_income"])["logsum_pop_high_income"]
-A_pop_logsum_low_income=orca.merge_tables(target='buildings', tables=['zones', 'parcels', 'buildings'], 
-                            columns=["logsum_pop_low_income"])["logsum_pop_low_income"]
-
-N_retail_jobs = orca.get_table('nodes_walk')['retail_jobs']
-
-
-
-# ========================
-P_property_tax=orca.get_table('parcels')['pptytax']
-
-
-# In[ ]:
-
-
-orca.get_table('zones').to_frame()
-
-
-# # Variables haven't been converted from OPUS(urbansim1) to ORCA(urbansim2)
-
-# In[ ]:
-
-
-## income
- I_disposable_inc = ln_bounded(household.income - (urbansim_parcel.building.unit_price/5.))
- "I_ln_income_less_price_per_unit = ln_bounded(household.income - ((urbansim_parcel.building.unit_price/10.) * urbansim_parcel.building.building_sqft_per_unit))",  
- #"I_ln_income_less_price_per_unit_x_is_condo_residential = ln_bounded(household.income - ((urbansim_parcel.building.unit_price/10.) * urbansim_parcel.building.building_sqft_per_unit)) * urbansim.building.is_condo_residential", # 
- "I_ln_income_less_price_per_unit_x_is_multi_family_residential = ln_bounded(household.income - (urbansim_parcel.building.unit_price/5.)) * washtenaw.building.is_multi_family_residential", # 
- "I_ln_income_less_price_per_unit_x_is_single_family_residential = ln_bounded(household.income - (urbansim_parcel.building.unit_price/5.)) * washtenaw.building.is_single_family_residential", I_ln_income_x_is_new_construction = ln(household.income) * (urbansim_parcel.building.age_masked < 2)
- I_ln_income_x_is_pre1945 = ln(household.income) * (urbansim_parcel.building.age_masked > 60)
- I_ln_income_x_is_single_family_residential = ln(household.income) * washtenaw.building.is_single_family_residential
- I_ln_income_x_ln_average_zonal_income = ln(household.income) * ln(building.disaggregate(urbansim_parcel.zone.average_income))
- I_ln_income_x_ln_lot_size_per_unit = ln(household.income) * ln((building.disaggregate(parcel.parcel_sqft)) / building.residential_units)
- I_ln_income_x_ln_lot_size_less_building_footprint_per_unit = ln(household.income) * ln(((building.disaggregate(parcel.parcel_sqft)) - building.land_area) / building.residential_units)
- I_ln_income_x_ln_lot_size_less_building_footprint_per_unit_x_is_single_family_residential = ln(household.income) * ln(((building.disaggregate(parcel.parcel_sqft)) - building.land_area) / building.residential_units) 
- I_ln_income_x_ln_price_per_sqft = ln(household.income) * ln(urbansim_parcel.building.unit_price)
- I_ln_income_x_ln_sqft_per_unit = ln(household.income) * ln(urbansim_parcel.building.building_sqft_per_unit)
- I_ln_income_x_ln_zonal_pop_den = ln(household.income) * (ln(building.disaggregate(urbansim_parcel.zone.population_per_acre))).astype(float32)
-I_ln_income_x_is_multi_family_residential = ln(household.income) * washtenaw.building.is_multi_family_residential
- I_ln_income_x_is_single_family_residential = ln(household.income) * washtenaw.building.is_single_family_residential
-I_is_high_income_x_is_single_family_residential=urbansim.household.is_high_income * washtenaw.building.is_single_family_residential
- I_is_mid_income_x_is_single_family_residential=urbansim.household.is_mid_income * washtenaw.building.is_single_family_residential
- I_is_high_income_x_school_quality=urbansim.household.is_high_income * building.disaggregate(school_district.proficient10, intermediates=[parcel])
- I_is_mid_income_x_school_quality=urbansim.household.is_mid_income * building.disaggregate(school_district.proficient10, intermediates=[parcel])
- I_is_low_income_x_school_quality=urbansim.household.is_low_income * building.disaggregate(school_district.proficient10, intermediates=[parcel])
- I_is_high_income_x_crime_rate=urbansim.household.is_high_income * building.disaggregate(city.rate_total, intermediates=[parcel])
- I_is_mid_income_x_crime_rate=urbansim.household.is_mid_income * building.disaggregate(city.rate_total, intermediates=[parcel])
- I_is_low_income_x_crime_rate=urbansim.household.is_low_income * building.disaggregate(city.rate_total, intermediates=[parcel])
- I_is_high_income_x_pptytax=urbansim.household.is_high_income * building.disaggregate(parcel.pptytax)
- I_is_mid_income_x_pptytax=urbansim.household.is_mid_income * building.disaggregate(parcel.pptytax)
- I_is_low_income_x_pptytax=urbansim.household.is_low_income * building.disaggregate(parcel.pptytax)
-  I_is_high_income_x_pptytax=urbansim.household.is_high_income * building.disaggregate(parcel.pptytax)
- I_is_mid_income_x_pptytax=urbansim.household.is_mid_income * building.disaggregate(parcel.pptytax)
- I_is_low_income_x_pptytax=urbansim.household.is_low_income * building.disaggregate(parcel.pptytax)
- I_is_high_income_x_percent_high_income=urbansim.household.is_high_income * building.disaggregate(washtenaw.zone.percent_high_income)
- I_is_mid_income_x_percent_mid_income=urbansim.household.is_mid_income * building.disaggregate(washtenaw.zone.percent_mid_income)
- I_is_low_income_x_percent_low_income=urbansim.household.is_low_income * building.disaggregate(washtenaw.zone.percent_low_income)
-
-## size
- I_hh_size_x_ln_zonal_pop_den = household.persons * (ln(building.disaggregate(urbansim_parcel.zone.population_per_acre))).astype(float32)
- I_hh_size_x_ln_sqft_per_unit = household.persons * ln(urbansim_parcel.building.sqft_per_unit)
-I_hh_size_3_x_single_family_residential=(household.persons> 2) *washtenaw.building.is_single_family_residential
- I_one_per_x_not_single_family_residential = (household.persons < 2) * numpy.logical_not(washtenaw.building.is_single_family_residential)
-
-## children
-I_has_children_x_ln_zonal_pop_den = (household.children > 0) * (ln(building.disaggregate(urbansim_parcel.zone.population_per_acre))).astype(float32)
- I_has_children_x_is_single_family_residential = (household.children > 0) * washtenaw.building.is_single_family_residential
- I_has_children_x_ln_sqft_per_unit = (household.children > 0) * ln(urbansim_parcel.building.sqft_per_unit)
- I_has_children_x_zonal_hh_with_children = (household.children > 0) * building.disaggregate(washtenaw.zone.percent_household_with_children)
- I_has_children_x_n_school_quality=(household.children > 0) * building.disaggregate(school_district.proficient10, intermediates=[parcel])
- I_has_children_x_crime_rate=(household.children > 0) *building.disaggregate(city.rate_total, intermediates=[parcel])
-
-## age
- I_is_young_x_ln_zonal_emp_den = urbansim.household.is_young * (ln(building.disaggregate(urbansim_parcel.zone.number_of_jobs_per_acre))).astype(float32)
- I_is_young_x_ln_zonal_number_of_jobs_of_sector_retail = urbansim.household.is_young * ln_bounded(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_7))
- I_is_young_x_ln_zonal_number_of_jobs_of_sector_retail_and_food_services = urbansim.household.is_young * ln_bounded(building.disaggregate(urbansim_parcel.zone.number_of_jobs_of_sector_7 + urbansim_parcel.zone.number_of_jobs_of_sector_14))
-
-
-## race
- I_is_race1_x_zonal_hh_race1=(household.race_id==1) * building.disaggregate(urbansim_parcel.zone.percent_household_race1)
- I_is_race2_x_zonal_hh_race2=(household.race_id==2) * building.disaggregate(urbansim_parcel.zone.percent_household_race2)
- I_is_race3_x_zonal_hh_race3=(household.race_id==3) * building.disaggregate(urbansim_parcel.zone.percent_household_race3)
- I_is_race4_x_zonal_hh_race4=(household.race_id==4) * building.disaggregate(urbansim_parcel.zone.percent_household_race4)
-### ACCESSIBILITY
-         
-A_has_workers_x_ln_emp_45min_hbw_drive_alone = (household.workers > 0) * building.disaggregate(ln_bounded(urbansim_parcel.zone.employment_within_45_minutes_travel_time_hbw_am_drive_alone))
-A_has_workers_x_ln_emp_45min_hbw_transit_walk = (household.workers > 0) * building.disaggregate(ln_bounded(urbansim_parcel.zone.employment_within_45_minutes_travel_time_hbw_am_transit_walk))
-A_ln_employment_within_45_minutes_travel_time_hbw_am_drive_alone = building.disaggregate(ln_bounded(urbansim_parcel.zone.employment_within_45_minutes_travel_time_hbw_am_drive_alone))
-A_ln_employment_within_45_minutes_travel_time_hbw_am_transit_walk = building.disaggregate(ln_bounded(urbansim_parcel.zone.employment_within_45_minutes_travel_time_hbw_am_transit_walk))
-A_logsum_accessibility_emp=(household.workers > household.cars)* (washtenaw.building.logsum_work_more_woker_than_car)+(household.workers <= household.cars)* (washtenaw.building.logsum_work_less_woker_than_car)
-A_logsum_accessibility_pop=washtenaw.building.logsum_pop_less_woker_than_car
-
-
-# # variables for future test
+Non
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/HLCM_variables.py to semcog_urbansim3/notebooks/others/HLCM_variables.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/REPM Estimation.py
--- semcog_urbansim/notebooks/others/REPM Estimation.py	(original)
+++ semcog_urbansim/notebooks/others/REPM Estimation.py	(refactored)
@@ -32,7 +32,7 @@
 
     b = orca.get_table('buildings').to_frame(model.columns_used()).fillna(0)
 
-    print model.fit(b)
+    print(model.fit(b))
     
     model.to_yaml(str_or_buffer=misc.config(yaml_config)) #  .replace('.yaml', '_new.yaml')
 
@@ -65,8 +65,8 @@
 
 
 for i, lcm in list(enumerate(repm_configs))[:]:
-    print 
-    print i, lcm
+    print() 
+    print(i, lcm)
     estimate_repm(lcm)
 
 
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/REPM Estimation.py to semcog_urbansim3/notebooks/others/REPM Estimation.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/accessibility_test.py
--- semcog_urbansim/notebooks/others/accessibility_test.py	(original)
+++ semcog_urbansim/notebooks/others/accessibility_test.py	(refactored)
@@ -14,7 +14,7 @@
     twoway=[1],
     impedances=None)
 
-print networks.NETWORKS.external_nodeids
+print(networks.NETWORKS.external_nodeids)
 
 dset = dataset.SemcogDataset("data/semcog_data.h5")
 
@@ -25,7 +25,7 @@
 dset.save_tmptbl("parcels", parcels)
 nodes = networks.from_yaml(dset, "networks.yaml")
 
-print networks.NETWORKS.external_nodeids
+print(networks.NETWORKS.external_nodeids)
 
 
 # In[1]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/accessibility_test.py to semcog_urbansim3/notebooks/others/accessibility_test.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/demo_model.py
--- semcog_urbansim/notebooks/others/demo_model.py	(original)
+++ semcog_urbansim/notebooks/others/demo_model.py	(refactored)
@@ -86,7 +86,7 @@
     
 @orca.step('print_year')
 def print_year(iter_var):
-    print '*** the year is {} ***'.format(iter_var)
+    print('*** the year is {} ***'.format(iter_var))
 
 
 # #### A demonstration of running the above models
@@ -119,9 +119,9 @@
 # In[ ]:
 
 
-print orca._TABLES
-print orca._MODELS
-print orca._INJECTABLES
+print(orca._TABLES)
+print(orca._MODELS)
+print(orca._INJECTABLES)
 
 ##show add_injectable and how it updates the _INJECTABLES list, then show the dictionaries up at top of sim (empty dicts)
 
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/demo_model.py to semcog_urbansim3/notebooks/others/demo_model.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/htransition_test.py
--- semcog_urbansim/notebooks/others/htransition_test.py	(original)
+++ semcog_urbansim/notebooks/others/htransition_test.py	(refactored)
@@ -69,13 +69,13 @@
 orca.run([
     "households_transition",  # households transition
 #     "households_relocation",  # households relocation model
-], iter_vars=range(2012, 2014))
+], iter_vars=list(range(2012, 2014)))
 
 
 # In[13]:
 
 
-print orca.get_injectable('iter_var')
+print(orca.get_injectable('iter_var'))
 hh_sim = orca.get_table('households').to_frame()
 
 store = orca.get_injectable('store')
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/htransition_test.py to semcog_urbansim3/notebooks/others/htransition_test.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/lcm_simulate.py
--- semcog_urbansim/notebooks/others/lcm_simulate.py	(original)
+++ semcog_urbansim/notebooks/others/lcm_simulate.py	(refactored)
@@ -42,7 +42,7 @@
 def choice_model_simulate(elcm1_model, jobs):
     choices = elcm1_model.simulate(choice_function=utils.unit_choices)
 
-    print 'There are %s unplaced agents.' % choices.isnull().sum()
+    print('There are %s unplaced agents.' % choices.isnull().sum())
 
     jobs.update_col_from_series(elcm1_model.choice_column, choices, cast=True)
 
@@ -62,7 +62,7 @@
 model.set_simulation_params('hlcm1', 'residential_units',
                             'vacant_residential_units', 'households', 'buildings', lids)
 
-print '** Score is %s' % model.score()
+print('** Score is %s' % model.score())
 
 
 # ## Plotting probabilities example
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/lcm_simulate.py to semcog_urbansim3/notebooks/others/lcm_simulate.py.
RefactoringTool: Refactored semcog_urbansim/notebooks/others/proforma_test.py
--- semcog_urbansim/notebooks/others/proforma_test.py	(original)
+++ semcog_urbansim/notebooks/others/proforma_test.py	(refactored)
@@ -110,7 +110,7 @@
 #     "gq_pop_scaling_model",
 #     "refiner",
 #     "travel_model",
-], iter_vars=range(2016, 2017), data_out=utils.get_run_filename(), out_interval=1)
+], iter_vars=list(range(2016, 2017)), data_out=utils.get_run_filename(), out_interval=1)
 
 
 # In[ ]:
@@ -543,7 +543,7 @@
 
 lacal = hf["buildings"]
 for c in lacal.columns:
-    print c, lacal[c].dtype
+    print(c, lacal[c].dtype)
     if lacal[c].dtype == np.float64:
         lacal[c] = lacal[c].astype(np.int32)
 del hf["buildings"]
@@ -559,8 +559,8 @@
 # In[ ]:
 
 
-for item in hf.keys():
-    print item, hf[item].shape
+for item in list(hf.keys()):
+    print(item, hf[item].shape)
 
 
 # In[ ]:
RefactoringTool: Writing converted semcog_urbansim/notebooks/others/proforma_test.py to semcog_urbansim3/notebooks/others/proforma_test.py.
RefactoringTool: Refactored semcog_urbansim/prep/cache_to_hdf5.py
--- semcog_urbansim/prep/cache_to_hdf5.py	(original)
+++ semcog_urbansim/prep/cache_to_hdf5.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-from __future__ import print_function
+
 
 import argparse
 import glob
RefactoringTool: Writing converted semcog_urbansim/prep/cache_to_hdf5.py to semcog_urbansim3/prep/cache_to_hdf5.py.
RefactoringTool: Refactored semcog_urbansim/variables/__init__.py
--- semcog_urbansim/variables/__init__.py	(original)
+++ semcog_urbansim/variables/__init__.py	(refactored)
@@ -1,7 +1,7 @@
 import dataset
-from variables_parcel import *
-from variables_access import *
-from variables_demographic import *
-from variables_employment import *
-from variables_zone import *
-from variables_building import *
+from .variables_parcel import *
+from .variables_access import *
+from .variables_demographic import *
+from .variables_employment import *
+from .variables_zone import *
+from .variables_building import *
RefactoringTool: Writing converted semcog_urbansim/variables/__init__.py to semcog_urbansim3/variables/__init__.py.
RefactoringTool: No changes to semcog_urbansim/variables/variables_access.py
RefactoringTool: Writing converted semcog_urbansim/variables/variables_access.py to semcog_urbansim3/variables/variables_access.py.
RefactoringTool: Refactored semcog_urbansim/variables/variables_building.py
--- semcog_urbansim/variables/variables_building.py	(original)
+++ semcog_urbansim/variables/variables_building.py	(refactored)
@@ -466,7 +466,7 @@
 
     @orca.column(to_geog_name, var_name, cache=True, cache_scope='iteration')
     def func():
-        print 'Disaggregating {} to {} from {}'.format(var_to_disaggregate, to_geog_name, from_geog_name)
+        print('Disaggregating {} to {} from {}'.format(var_to_disaggregate, to_geog_name, from_geog_name))
 
         from_geog = orca.get_table(from_geog_name)
         to_geog = orca.get_table(to_geog_name)
RefactoringTool: Writing converted semcog_urbansim/variables/variables_building.py to semcog_urbansim3/variables/variables_building.py.
RefactoringTool: Refactored semcog_urbansim/variables/variables_demographic.py
--- semcog_urbansim/variables/variables_demographic.py	(original)
+++ semcog_urbansim/variables/variables_demographic.py	(refactored)
@@ -47,7 +47,7 @@
 def household_type(households, household_type_map):
     df = households.to_frame(['income_quartile', 'age_of_head', 'persons'])
     df['household_type'] = 0
-    for i, q in household_type_map.iteritems():
+    for i, q in household_type_map.items():
         idx = df.query(q).index.values
         df.loc[idx, 'household_type']= i
     return df.household_type.fillna(0)
RefactoringTool: Writing converted semcog_urbansim/variables/variables_demographic.py to semcog_urbansim3/variables/variables_demographic.py.
RefactoringTool: No changes to semcog_urbansim/variables/variables_employment.py
RefactoringTool: Writing converted semcog_urbansim/variables/variables_employment.py to semcog_urbansim3/variables/variables_employment.py.
RefactoringTool: Refactored semcog_urbansim/variables/variables_parcel.py
--- semcog_urbansim/variables/variables_parcel.py	(original)
+++ semcog_urbansim/variables/variables_parcel.py	(refactored)
@@ -101,7 +101,7 @@
     if form:
         columns = ['type%d' % typ for typ in form_to_btype[form]]
     else:
-        columns = ['type%d' % typ for typ in set(item for sublist in form_to_btype.values() for item in sublist)]
+        columns = ['type%d' % typ for typ in set(item for sublist in list(form_to_btype.values()) for item in sublist)]
 
     allowed = zoning.to_frame(columns).max(axis=1).reindex(index, fill_value=0)
 
RefactoringTool: Writing converted semcog_urbansim/variables/variables_parcel.py to semcog_urbansim3/variables/variables_parcel.py.
RefactoringTool: Refactored semcog_urbansim/variables/variables_zone.py
--- semcog_urbansim/variables/variables_zone.py	(original)
+++ semcog_urbansim/variables/variables_zone.py	(refactored)
@@ -26,7 +26,7 @@
 
 @orca.column('zones', cache=True, cache_scope='iteration')
 def households(households):
-    print type(households)
+    print(type(households))
     return households.zone_id.groupby(households.zone_id).size()
 
 
@@ -65,7 +65,7 @@
     unique_zone_ids = np.unique(zones.zone_id.values)
 
     zones.index = zones.index.values + 1
-    zone_id_xref = dict(zip(zones.zone_id, zones.index.values))
+    zone_id_xref = dict(list(zip(zones.zone_id, zones.index.values)))
     apply_xref = lambda x: zone_id_xref[x]
 
     td = td[td.from_zone_id.isin(unique_zone_ids)]
@@ -246,7 +246,7 @@
 
     @orca.column(to_geog_name, var_name, cache=True, cache_scope='iteration')
     def func():
-        print 'Disaggregating {} to {} from {}'.format(var_to_disaggregate, to_geog_name, from_geog_name)
+        print('Disaggregating {} to {} from {}'.format(var_to_disaggregate, to_geog_name, from_geog_name))
 
         from_geog = orca.get_table(from_geog_name)
         to_geog = orca.get_table(to_geog_name)
RefactoringTool: Writing converted semcog_urbansim/variables/variables_zone.py to semcog_urbansim3/variables/variables_zone.py.
RefactoringTool: Files that were modified:
RefactoringTool: semcog_urbansim/Cost_Shift_test.py
RefactoringTool: semcog_urbansim/Refiner.py
RefactoringTool: semcog_urbansim/Simulation.py
RefactoringTool: semcog_urbansim/Simulation_no_scaling.py
RefactoringTool: semcog_urbansim/assumptions.py
RefactoringTool: semcog_urbansim/dataset.py
RefactoringTool: semcog_urbansim/lcm_utils.py
RefactoringTool: semcog_urbansim/models.py
RefactoringTool: semcog_urbansim/output_indicators.py
RefactoringTool: semcog_urbansim/output_indicators_run_test.py
RefactoringTool: semcog_urbansim/proforma_generate_settings.py
RefactoringTool: semcog_urbansim/proforma_run_test.py
RefactoringTool: semcog_urbansim/refiner_run_test.py
RefactoringTool: semcog_urbansim/transcad.py
RefactoringTool: semcog_urbansim/utils.py
RefactoringTool: semcog_urbansim/verify_data_structure.py
RefactoringTool: semcog_urbansim/data/fix_base_hh.py
RefactoringTool: semcog_urbansim/data/fix_city3130_hhsize-Copy1.py
RefactoringTool: semcog_urbansim/data/fix_city3130_hhsize.py
RefactoringTool: semcog_urbansim/notebooks/Location Choice Model.py
RefactoringTool: semcog_urbansim/notebooks/Proforma Demo.py
RefactoringTool: semcog_urbansim/notebooks/Residential Price Model.py
RefactoringTool: semcog_urbansim/notebooks/accessibility_w_local.py
RefactoringTool: semcog_urbansim/notebooks/refine by changing hh pop.py
RefactoringTool: semcog_urbansim/notebooks/refine by hh swap.py
RefactoringTool: semcog_urbansim/notebooks/shape2hdf5_w_local.py
RefactoringTool: semcog_urbansim/notebooks/shape2net.py
RefactoringTool: semcog_urbansim/notebooks/shape2net_w_localnodes.py
RefactoringTool: semcog_urbansim/notebooks/shape2pickle.py
RefactoringTool: semcog_urbansim/notebooks/model_analysis/ELCM_Diagnostics.py
RefactoringTool: semcog_urbansim/notebooks/model_analysis/HLCM_follow-up_Diagnostics.py
RefactoringTool: semcog_urbansim/notebooks/model_analysis/confirm_coeffs_with_choicemodels.py
RefactoringTool: semcog_urbansim/notebooks/others/ELCM Estimation.py
RefactoringTool: semcog_urbansim/notebooks/others/ELCM_variables.py
RefactoringTool: semcog_urbansim/notebooks/others/Exploration.py
RefactoringTool: semcog_urbansim/notebooks/others/HLCM Estimation.py
RefactoringTool: semcog_urbansim/notebooks/others/HLCM_variables.py
RefactoringTool: semcog_urbansim/notebooks/others/REPM Estimation.py
RefactoringTool: semcog_urbansim/notebooks/others/accessibility_test.py
RefactoringTool: semcog_urbansim/notebooks/others/demo_model.py
RefactoringTool: semcog_urbansim/notebooks/others/htransition_test.py
RefactoringTool: semcog_urbansim/notebooks/others/lcm_simulate.py
RefactoringTool: semcog_urbansim/notebooks/others/proforma_test.py
RefactoringTool: semcog_urbansim/prep/cache_to_hdf5.py
RefactoringTool: semcog_urbansim/variables/__init__.py
RefactoringTool: semcog_urbansim/variables/variables_access.py
RefactoringTool: semcog_urbansim/variables/variables_building.py
RefactoringTool: semcog_urbansim/variables/variables_demographic.py
RefactoringTool: semcog_urbansim/variables/variables_employment.py
RefactoringTool: semcog_urbansim/variables/variables_parcel.py
RefactoringTool: semcog_urbansim/variables/variables_zone.py
RefactoringTool: There were 6 errors:
RefactoringTool: Can't parse semcog_urbansim/data/fix_base_hh.py: ParseError: bad input: type=1, value='by', context=(' ', (115, 16))
RefactoringTool: Can't parse semcog_urbansim/data/fix_city3130_hhsize-Copy1.py: ParseError: bad input: type=1, value='by', context=(' ', (157, 16))
RefactoringTool: Can't parse semcog_urbansim/data/fix_city3130_hhsize.py: ParseError: bad input: type=1, value='by', context=(' ', (115, 16))
RefactoringTool: Can't parse semcog_urbansim/notebooks/Proforma Demo.py: ParseError: bad input: type=3, value="'unit_price_nonres > 0'", context=('                    ', (161, 21))
RefactoringTool: Can't parse semcog_urbansim/notebooks/others/ELCM_variables.py: ParseError: bad input: type=4, value='\n', context=('', (204, 99))
RefactoringTool: Can't parse semcog_urbansim/notebooks/others/HLCM_variables.py: ParseError: bad input: type=5, value=' ', context=("\n\n# # Variables haven't been converted from OPUS(urbansim1) to ORCA(urbansim2)\n\n# In[ ]:\n\n\n## income\n", (289, 0))
(base) da@da-virtual-machine:~$ 
